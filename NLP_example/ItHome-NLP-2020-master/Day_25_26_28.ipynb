{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 25: 猜字遊戲"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "語言模型可以以字詞為單位作N-gram，也可以以字母為單位。為了讓大家對語言模型有個更清楚的理解，我們用英文猜字遊戲Hangman為範例來學習。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 猜字遊戲Hangman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://en.wikipedia.org/wiki/Hangman_(game)\">Hangman game</a>的玩法是透過一個人寫下一個字，讓另一個人來猜。對方一次會猜一個英文字母，猜對的話就把字母寫到正確的格子上，猜錯的話則會記錄下來。若是猜錯的次數達到一個數字，則猜字的一方輸。\n",
    "\n",
    "這裡我們先開發一個簡易版："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hangman(secret_word, guesser, max_mistakes=8, verbose=True, **guesser_args):\n",
    "    \"\"\"\n",
    "        secret_word是將要被猜的字、guesser是我們之後會陸續寫進來的猜字模型（真人猜字或AI）、max_mistakes是最多可以錯誤的次數、\n",
    "        verbose為True時表示互動性猜字（AI猜字時可以改False）、guesser_args是一個keyword argument。\n",
    "    \"\"\"\n",
    "    secret_word = secret_word.lower()\n",
    "    mask = ['_'] * len(secret_word) # 把要被猜的字轉成暗文\n",
    "    guessed = set()\n",
    "    if verbose:\n",
    "        print(\"開始猜字遊戲，提示：\", ' '.join(mask), '長度為', len(secret_word))\n",
    "    \n",
    "    mistakes = 0\n",
    "    while mistakes < max_mistakes:\n",
    "        if verbose:\n",
    "            print(\"你還有\", (max_mistakes-mistakes), \"次機會。\")\n",
    "        guess = guesser(mask, guessed, **guesser_args)\n",
    "\n",
    "        if verbose:\n",
    "            print('你猜了：', guess)\n",
    "        if guess in guessed:\n",
    "            if verbose:\n",
    "                print('這個字母已經猜過了！')\n",
    "            mistakes += 1\n",
    "        else:\n",
    "            guessed.add(guess)\n",
    "            if guess in secret_word:\n",
    "                for i, c in enumerate(secret_word):\n",
    "                    if c == guess:\n",
    "                        mask[i] = c\n",
    "                if verbose:\n",
    "                    print('猜對了，', ' '.join(mask))\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print('抱歉，再猜猜')\n",
    "                mistakes += 1\n",
    "                \n",
    "        if '_' not in mask:\n",
    "            if verbose:\n",
    "                print('恭喜你贏了！')\n",
    "            return mistakes\n",
    "        \n",
    "    if verbose:\n",
    "        print('沒有機會了，正確字是', secret_word)    \n",
    "    return mistakes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這裡我們先寫一個人機互動猜字版："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def human(mask, guessed, **kwargs):\n",
    "    \"\"\"\n",
    "    可以手動遊玩\n",
    "    \"\"\"\n",
    "    print('請輸入你要猜的字：')\n",
    "    return input().lower().strip()\n",
    "\n",
    "interactive = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "試玩遊戲："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "開始猜字遊戲，提示： _ _ _ _ _ _ _ _ _ 長度為 9\n",
      "你還有 8 次機會。\n",
      "請輸入你要猜的字：\n"
     ]
    }
   ],
   "source": [
    "if interactive:\n",
    "    hangman('algorithm', human, 8, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 準備訓練和測試集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "開始寫我們的AI猜字之前，要先準備訓練集和測試集。從[這篇文章](http://datagenetics.com/blog/april12012/index.html)中可以看到，照著統計學來猜字是個不錯的做法，因此我們也自己來訓練一組模型，照著出現的頻率為順序進行猜字。\n",
    "\n",
    "我們使用之前用過的*Brown Corpus*來訓練AI猜字演算法，為了刻意增加猜字的難度，AI在測試時的字和訓練的字會是不一樣的(把訓練集和測試集拆開)，如此才能訓練出更通用的模型。\n",
    "\n",
    "我們使用 `nltk.corpus.Brown` 裡的 `words()` 方法，從中將非英文字母的文字去除，同時將每個字小寫化。之後，我們用 `numpy.random.shuffle` 來隨機刷取文集中的字，以分成訓練集和測試集。測試集裡有1000字，剩下的字都會到訓練集裡面。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40234\n",
      "1000\n",
      "39234\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(12345)\n",
    "\n",
    "# word_set 儲存Brown Corpus中所有獨特的字型\n",
    "word_set = []\n",
    "# test_set 儲存1000個字的測試集\n",
    "test_set = []\n",
    "# training_set 將剩下的字存到訓練集中\n",
    "training_set = []\n",
    "\n",
    "\n",
    "word_s = set([])\n",
    "\n",
    "for word in brown.words():\n",
    "    if word.isalpha():\n",
    "        word = word.lower()\n",
    "        if word not in word_s:\n",
    "            word_s.add(word)\n",
    "\n",
    "word_set = list(word_s)\n",
    "np.random.shuffle(word_set)\n",
    "\n",
    "test_set = word_set[:1000]\n",
    "training_set = word_set[1000:]\n",
    "\n",
    "print(len(word_set))\n",
    "print(len(test_set))\n",
    "print(len(training_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這裡一來就不是一個人寫一個字讓對方猜了，而是電腦和人類的對決："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "開始猜字遊戲，提示： _ _ _ _ _ _ 長度為 6\n",
      "你還有 2 次機會。\n",
      "請輸入你要猜的字：\n",
      "q\n",
      "你猜了： q\n",
      "抱歉，再猜猜\n",
      "你還有 1 次機會。\n",
      "請輸入你要猜的字：\n",
      "z\n",
      "你猜了： z\n",
      "抱歉，再猜猜\n",
      "沒有機會了，正確字是 umpire\n"
     ]
    }
   ],
   "source": [
    "if interactive:\n",
    "    hangman(np.random.choice(test_set), human, 8, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 26 三種AI猜字方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第一種猜字方法：隨機猜字\n",
    "\n",
    "為了設下一個基準，我們先設計一種*AI*方法--每次從26個字母中隨機選取一個字母來猜。這裡我先將26個字母存到 `list` 中，再用 `numpy.random.choice` 隨機選取。\n",
    "\n",
    "`test_guesser` 是用來測試平均猜錯次數的方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_guesser(guesser, test=test_set):\n",
    "    \"\"\"\n",
    "        這個方法是用來測試平均猜錯的次數。\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    for word in test:\n",
    "        total += hangman(word, guesser, 26, False)\n",
    "    return total / float(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "平均錯誤次數： 16.453\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def random_guesser(mask, guessed, **kwargs):\n",
    "    \"\"\"\n",
    "        隨機猜字\n",
    "    \"\"\"\n",
    "\n",
    "    alphabets = []\n",
    "    for letter in range(97,123):\n",
    "        if chr(letter) not in guessed:\n",
    "            alphabets.append(chr(letter))\n",
    "\n",
    "    picked = np.random.choice(alphabets)\n",
    "    return picked\n",
    "\n",
    "# 若要看看機器是怎麼猜字的，可以把下面這句打開\n",
    "#hangman(np.random.choice(test_set), random_guesser, 10, True)\n",
    "\n",
    "result = test_guesser(random_guesser)\n",
    "print()\n",
    "print(\"平均錯誤次數：\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第二種猜法：Unigram Guesser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們可以嘗試用*Unigram*模型來訓練。我們需要知道每個字母的出現頻率，接著照出現頻率的高低來進行猜字。每當猜完一個字之後就應該把猜過的字去掉。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'e': 35207, 'i': 26008, 'a': 24361, 's': 23721, 'n': 22658, 'r': 22054, 't': 20665, 'o': 19167, 'l': 16670, 'c': 12626, 'd': 12267, 'u': 10185, 'm': 8668, 'g': 8609, 'p': 8563, 'h': 7342, 'b': 5724, 'y': 5363, 'f': 4216, 'v': 3442, 'k': 2978, 'w': 2792, 'z': 1040, 'x': 835, 'j': 641, 'q': 531})\n",
      "\n",
      "平均猜錯次數： 10.398\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# unigram_counts 儲存了整個訓練及中每個字母的出現次數\n",
    "unigram_counts = Counter()\n",
    "\n",
    "for word in training_set:\n",
    "    for letter in word:\n",
    "        unigram_counts[letter] += 1\n",
    "\n",
    "print(unigram_counts)\n",
    "\n",
    "\n",
    "def unigram_guesser(mask, guessed, unigram_counts=unigram_counts):\n",
    "    \"\"\"\n",
    "        這個方法實作了Unigram Guesser，會根據Unigram Model每次回傳一個要猜的字。\n",
    "    \"\"\"\n",
    "    \n",
    "    unigram_keys = []\n",
    "\n",
    "    # 照出現頻率將字母排序\n",
    "    for i in range(len(unigram_counts)):\n",
    "        unigram_keys.append(unigram_counts.most_common()[i][0])\n",
    "\n",
    "    # 將猜過的字去除\n",
    "    for letter in guessed:\n",
    "        if letter in unigram_keys:\n",
    "            unigram_keys.remove(letter)\n",
    "\n",
    "    return unigram_keys[0]\n",
    "\n",
    "#hangman(np.random.choice(test_set), unigram_guesser, 10, True)\n",
    "\n",
    "result = test_guesser(unigram_guesser)\n",
    "print()\n",
    "print(\"平均猜錯次數：\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第三種猜法：根據文字長度猜字"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "從和昨天[同一篇文章](http://datagenetics.com/blog/april12012/index.html)中我們看到，不同的文字長度，每個字母出現的頻率不盡相同，例如，短的字比較不會出現前綴或後綴。在這裡，我們針對不同的文字長度設計不一樣的猜字順序。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'collections.Counter'>, {3: Counter({'a': 238, 'e': 198, 'o': 180, 'i': 130, 'n': 120, 't': 116, 's': 115, 'p': 106, 'm': 103, 'u': 102, 'd': 100, 'r': 97, 'b': 89, 'l': 79, 'h': 78, 'g': 78, 'y': 76, 'w': 72, 'c': 70, 'f': 51, 'k': 32, 'j': 28, 'v': 25, 'x': 23, 'z': 10, 'q': 6}), 7: Counter({'e': 5324, 'a': 3539, 's': 3471, 'r': 3413, 'i': 3332, 'n': 3049, 't': 2663, 'o': 2500, 'l': 2475, 'd': 2051, 'c': 1660, 'u': 1506, 'g': 1500, 'm': 1234, 'p': 1222, 'h': 1098, 'b': 942, 'y': 749, 'f': 675, 'k': 578, 'w': 524, 'v': 464, 'z': 154, 'x': 107, 'j': 95, 'q': 83}), 9: Counter({'e': 5401, 'i': 3903, 'n': 3468, 's': 3442, 'a': 3432, 'r': 3235, 't': 3067, 'o': 2606, 'l': 2301, 'c': 1911, 'd': 1858, 'u': 1427, 'g': 1361, 'm': 1241, 'p': 1147, 'h': 1027, 'b': 756, 'y': 643, 'f': 578, 'v': 530, 'k': 368, 'w': 367, 'z': 133, 'x': 122, 'q': 85, 'j': 69}), 6: Counter({'e': 4251, 'a': 2697, 's': 2597, 'r': 2535, 'i': 2040, 'n': 1999, 'o': 1974, 'l': 1958, 't': 1856, 'd': 1620, 'u': 1144, 'c': 1070, 'm': 936, 'g': 881, 'h': 876, 'p': 861, 'b': 772, 'y': 717, 'f': 502, 'k': 455, 'w': 429, 'v': 398, 'z': 125, 'j': 104, 'x': 97, 'q': 52}), 11: Counter({'e': 2950, 'i': 2747, 'n': 2268, 't': 2156, 'a': 2013, 's': 2003, 'r': 1921, 'o': 1706, 'l': 1322, 'c': 1236, 'u': 862, 'd': 806, 'p': 778, 'm': 739, 'g': 630, 'h': 517, 'b': 418, 'y': 407, 'f': 291, 'v': 290, 'k': 134, 'w': 121, 'x': 74, 'z': 71, 'q': 46, 'j': 26}), 10: Counter({'e': 4361, 'i': 3647, 'n': 3104, 's': 2920, 't': 2815, 'a': 2810, 'r': 2685, 'o': 2404, 'l': 1960, 'c': 1682, 'd': 1404, 'u': 1304, 'g': 1095, 'p': 1090, 'm': 1065, 'h': 822, 'b': 622, 'y': 558, 'f': 462, 'v': 448, 'w': 209, 'k': 179, 'z': 112, 'x': 98, 'q': 67, 'j': 57}), 4: Counter({'e': 903, 'a': 869, 's': 695, 'o': 685, 'l': 559, 'i': 541, 'r': 519, 't': 488, 'n': 478, 'd': 364, 'u': 348, 'm': 298, 'p': 290, 'h': 288, 'b': 260, 'c': 254, 'k': 231, 'g': 219, 'w': 194, 'y': 176, 'f': 171, 'v': 98, 'j': 63, 'z': 46, 'x': 35, 'q': 8}), 5: Counter({'e': 2193, 's': 1808, 'a': 1806, 'r': 1358, 'o': 1323, 'l': 1189, 'i': 1144, 't': 1076, 'n': 1030, 'd': 767, 'c': 695, 'u': 667, 'h': 593, 'm': 564, 'y': 545, 'p': 522, 'b': 496, 'g': 458, 'k': 377, 'f': 343, 'w': 303, 'v': 235, 'z': 87, 'j': 76, 'x': 66, 'q': 29}), 12: Counter({'i': 1889, 'e': 1885, 'n': 1539, 't': 1458, 'a': 1417, 's': 1305, 'r': 1244, 'o': 1229, 'l': 968, 'c': 902, 'u': 590, 'p': 553, 'm': 521, 'd': 502, 'g': 390, 'h': 383, 'y': 299, 'b': 252, 'f': 198, 'v': 195, 'z': 72, 'k': 53, 'w': 52, 'x': 38, 'q': 37, 'j': 17}), 8: Counter({'e': 5628, 'i': 3886, 's': 3692, 'a': 3677, 'n': 3515, 'r': 3492, 't': 2893, 'o': 2772, 'l': 2624, 'd': 2236, 'c': 1956, 'u': 1575, 'g': 1531, 'm': 1290, 'p': 1242, 'h': 1165, 'b': 852, 'y': 714, 'f': 713, 'v': 541, 'k': 511, 'w': 473, 'z': 115, 'x': 109, 'j': 85, 'q': 81}), 2: Counter({'a': 25, 'e': 21, 'o': 19, 'm': 19, 's': 19, 'i': 15, 'l': 13, 'u': 11, 't': 11, 'n': 11, 'h': 11, 'c': 11, 'd': 11, 'f': 10, 'p': 9, 'b': 9, 'r': 9, 'w': 7, 'v': 6, 'g': 6, 'y': 6, 'j': 5, 'k': 4, 'x': 3, 'q': 2, 'z': 1}), 14: Counter({'i': 716, 'e': 560, 'n': 540, 't': 531, 'a': 463, 's': 454, 'o': 448, 'r': 395, 'l': 331, 'c': 304, 'p': 176, 'u': 172, 'm': 156, 'd': 149, 'h': 128, 'y': 119, 'g': 116, 'b': 71, 'v': 64, 'f': 63, 'z': 34, 'x': 12, 'k': 11, 'w': 10, 'q': 8, 'j': 3}), 13: Counter({'i': 1287, 'e': 1054, 'n': 1035, 't': 975, 'a': 915, 'o': 849, 's': 808, 'r': 750, 'c': 572, 'l': 553, 'p': 364, 'm': 351, 'u': 337, 'd': 259, 'g': 248, 'y': 209, 'h': 207, 'b': 142, 'v': 117, 'f': 112, 'z': 37, 'x': 35, 'k': 28, 'w': 23, 'q': 20, 'j': 10}), 1: Counter({'s': 1, 'h': 1, 'w': 1, 'v': 1, 'q': 1, 'n': 1, 'c': 1, 'r': 1, 'e': 1, 'u': 1, 'y': 1, 'o': 1, 'x': 1, 'z': 1, 'b': 1, 't': 1, 'd': 1, 'm': 1, 'f': 1, 'p': 1, 'k': 1, 'l': 1, 'g': 1, 'j': 1, 'a': 1, 'i': 1}), 15: Counter({'i': 397, 't': 282, 'n': 273, 'o': 250, 'e': 249, 'a': 235, 's': 213, 'r': 200, 'l': 167, 'c': 167, 'p': 110, 'y': 79, 'u': 74, 'm': 74, 'd': 73, 'h': 69, 'g': 51, 'f': 26, 'b': 23, 'v': 19, 'z': 18, 'k': 8, 'x': 8, 'w': 5, 'q': 4, 'j': 1}), 18: Counter({'i': 48, 't': 44, 'e': 37, 's': 35, 'r': 34, 'o': 33, 'n': 29, 'a': 25, 'c': 24, 'l': 23, 'p': 14, 'h': 13, 'd': 12, 'y': 10, 'm': 9, 'u': 8, 'g': 6, 'f': 4, 'k': 2, 'b': 2, 'v': 1, 'z': 1}), 16: Counter({'i': 167, 't': 132, 'n': 110, 'a': 109, 'e': 105, 'o': 102, 'r': 90, 'l': 90, 's': 85, 'c': 61, 'p': 45, 'm': 35, 'h': 35, 'y': 32, 'u': 31, 'd': 27, 'g': 22, 'b': 11, 'z': 10, 'f': 10, 'x': 6, 'v': 6, 'k': 4, 'q': 2, 'w': 1}), 17: Counter({'i': 95, 't': 82, 'a': 72, 'e': 71, 'n': 68, 'r': 64, 'o': 59, 's': 45, 'l': 43, 'c': 39, 'p': 27, 'm': 26, 'h': 26, 'd': 24, 'y': 20, 'u': 18, 'g': 12, 'z': 9, 'b': 5, 'v': 4, 'f': 3, 'w': 1, 'k': 1, 'x': 1, 'j': 1}), 20: Counter({'i': 5, 't': 4, 'n': 3, 'o': 2, 'a': 2, 's': 1, 'u': 1, 'l': 1, 'z': 1}), 21: Counter({'o': 6, 'c': 4, 'i': 3, 'm': 3, 'e': 3, 'l': 3, 'r': 3, 'p': 3, 'h': 3, 's': 3, 'a': 3, 'u': 1, 'n': 1, 't': 1, 'y': 1, 'g': 1}), 19: Counter({'o': 18, 'i': 15, 'n': 14, 't': 13, 'a': 11, 'r': 9, 'e': 8, 's': 7, 'l': 7, 'c': 7, 'u': 5, 'p': 3, 'd': 3, 'm': 3, 'g': 3, 'f': 2, 'h': 2, 'z': 2, 'y': 1}), 22: Counter({'e': 4, 'l': 3, 'n': 3, 'a': 2, 's': 2, 'k': 1, 'y': 1, 'b': 1, 'z': 1, 'u': 1, 'f': 1, 'o': 1, 't': 1})})\n",
      "\n",
      "平均猜錯次數： 10.377\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# unigram_counts_by_length 將文字長度和字母頻率map在一起\n",
    "unigram_counts_by_length = defaultdict(Counter)\n",
    "\n",
    "# 幫每一種文字長度寫不同的Unigram Model\n",
    "for word in training_set:    \n",
    "    this_count = Counter()\n",
    "    for letter in word:\n",
    "        this_count[letter] += 1\n",
    "        unigram_counts_by_length[len(word)] += this_count\n",
    "        this_count = Counter()\n",
    "\n",
    "        \n",
    "def exclude_guessed_letters(length_model, guessed):\n",
    "    unigram_keys_by_length = []\n",
    "    # 照出現頻率將字母排序\n",
    "    for i in range(len(unigram_counts_by_length[length_model])):\n",
    "        unigram_keys_by_length.append(unigram_counts_by_length[length_model].most_common()[i][0])\n",
    "    \n",
    "    # 將猜過的字去除\n",
    "    for letter in guessed:\n",
    "        if letter in unigram_keys_by_length:\n",
    "            unigram_keys_by_length.remove(letter)\n",
    "    \n",
    "    return unigram_keys_by_length\n",
    "\n",
    "\n",
    "lengths = sorted(unigram_counts_by_length.keys())\n",
    "max_length = lengths[-1] + 1\n",
    "\n",
    "print(unigram_counts_by_length)\n",
    "\n",
    "def unigram_length_guesser(mask, guessed, counts=unigram_counts_by_length):\n",
    "    \n",
    "    length_model = len(mask)\n",
    "    # 若要猜的文字長度不在unigram model時，我們將一長度來猜。\n",
    "    while length_model not in lengths:\n",
    "        length_model -= 1\n",
    "    \n",
    "    unigram_keys_by_length = exclude_guessed_letters(length_model, guessed)\n",
    "    \n",
    "    # 若這個文字長度沒有猜字選項了，從附近的文字長度找\n",
    "    while len(unigram_keys_by_length) == 0:\n",
    "        if length_model < 20:\n",
    "            length_model += 1\n",
    "        else:\n",
    "            length_model -= 1\n",
    "        unigram_keys_by_length = exclude_guessed_letters(length_model, guessed)\n",
    "    \n",
    "    return unigram_keys_by_length[0]\n",
    "\n",
    "\n",
    "#hangman(np.random.choice(test_set), unigram_length_guesser, 10, True)\n",
    "\n",
    "result = test_guesser(unigram_length_guesser)\n",
    "print()\n",
    "print(\"平均猜錯次數：\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 28: 猜字加強版 -- Bigram Guesser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了考慮字母出現的機率和在各種長度中各個字母出現的機率，今天我們也把字母的排列順序列入考量。例如，我們看到一個字 `m _ s s`，我們知道他有很高的機率會是母音，因此，`a`, `e`, `i`, `o`, `u`就是我們下一個猜測的目標。\n",
    "\n",
    "在這裡，我們開發一個Bigram的語言模型。在開始訓練這個模型之前，有幾件事需要注意：(1) 要記得加上開頭標籤 `$`；(2) 要記得針對Bigram以上的模型設計對應的平滑方法。在這裡我們決定使用 *linear interpolation （線性插值法）*來smooth高元和低元模型，而插值法中要用到的 \\lambda 值我們設為0.75（建議在0.7~0.8之間）。\n",
    "\n",
    "我們猜格子的順序會從最左邊開始猜，因此最開始會在p($)的條件下猜字母。猜的過程中可能第一個字還沒猜到，後面有些就先被猜到了，等我們終於猜到第一個字母後，我們就找下一個「最左邊還沒有被猜到的字」。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "平均猜錯次數： 9.054\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "bigram_counts = defaultdict(Counter) # 這個dict的key是各個字母，value是可能接在這個字母後面之字母的Counter\n",
    "\n",
    "bigram_inter = defaultdict(list) # 根據interpolation後的機率，將接續在一個字母後面的可能字母照出現機率排序\n",
    "\n",
    "# 開始儲存Bigram model\n",
    "for word in training_set:\n",
    "    this_count = Counter()\n",
    "    word = '$' + word # 字首加上開頭標籤\n",
    "    for i, letter in enumerate(word):\n",
    "        if i+1 != len(word):\n",
    "            this_count[word[i+1]] += 1\n",
    "            bigram_counts[letter] += this_count\n",
    "            this_count = Counter()\n",
    "\n",
    "set_lambda = 0.75 # 設定interpolation的lambda\n",
    "\n",
    "# Bigram Interpolated Model: p = lambda*p(w_i|w_(i-1)) + (1-lambda)*p(w_i)\n",
    "# p(w_i|w_(i-1)) = count(w_i and w_(i-1)) / count(w_(i-1))\n",
    "# p(w_i) = count(w_i) / sigma(count(w_i))\n",
    "for key in bigram_counts.keys():\n",
    "    sigma_count_wi = sum(unigram_counts.values()) # sigma(count(w_i))\n",
    "    count_wi1 = sum(bigram_counts[key].values()) # count(w_(i-1))\n",
    "\n",
    "    # 計算p(w_i|w_(i-1))和p(w_i)\n",
    "    prob_of_letter = {}\n",
    "    for letter in range(97,123):\n",
    "        p_wi_wi1 = bigram_counts[key][chr(letter)] / count_wi1 # p(w_i|w_(i-1))\n",
    "        p_wi = unigram_counts[chr(letter)] / sigma_count_wi # p(w_i)\n",
    "        prob_of_letter[chr(letter)] = set_lambda*p_wi_wi1 + (1-set_lambda)*p_wi\n",
    "\n",
    "    # 將接續在一個字母後面的可能字母照出現機率排序\n",
    "    this_list = []\n",
    "    for i in range(26):\n",
    "        this_list.append(sorted(prob_of_letter.items(), key=itemgetter(1), reverse=True)[i][0])\n",
    "    bigram_inter[key] = this_list\n",
    "    \n",
    "\n",
    "def bigram_guesser(mask, guessed, counts=bigram_counts): # add extra default arguments if needed\n",
    "    \"\"\"\n",
    "        實現Bigram Guesser的方法，根據使用了線性插值法的Bigram model，回傳猜測的字母。\n",
    "    \"\"\"\n",
    "    \n",
    "    mask = ['$'] + mask\n",
    "    w_i_1 = \"\" # 找到最左邊非 '_' 的字母 -> w_(i-1)\n",
    "    # 找w_i_1\n",
    "    for i in range(len(mask)):\n",
    "        if mask[i] == '_':\n",
    "            w_i_1 = mask[i-1]\n",
    "            break\n",
    "    copy_bi_model = bigram_inter[w_i_1].copy()\n",
    "    \n",
    "    # 將已經猜過的字母去除\n",
    "    for letter in guessed:\n",
    "        if letter in copy_bi_model:\n",
    "            copy_bi_model.remove(letter)\n",
    "\n",
    "    return copy_bi_model[0]\n",
    "\n",
    "\n",
    "#hangman(np.random.choice(test_set), bigram_guesser, 26, True)\n",
    "\n",
    "result = test_guesser(bigram_guesser)\n",
    "print()\n",
    "print(\"平均猜錯次數：\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
