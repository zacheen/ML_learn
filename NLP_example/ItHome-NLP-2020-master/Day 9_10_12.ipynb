{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這裡面就是統計這篇文章 正規化後各個字的數量\n",
    "\n",
    "用NLTK的工具word_tokenize來記號化每份文件\n",
    "用PorterStemmer來stem和小寫化(lowercase)各文件，並且把這些正規化後的字彙加上Unique ID存進Python的dict資料型態\n",
    "    所有的字彙M有自己的字彙ID[0..M−1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 9: 倒排索引"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 預處理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今天的實作我們會使用華爾街日報的的文件集，我有預先將文件集切割成只有兩萬份文件的集合，這份文件集能夠從以下的code中下載。在今天的實作中，我們會將每一行視為一份文件來處理（利用NLTK工具來記號化和正規劃）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "fname = 'wsta_col_20k.gz'\n",
    "my_file = Path(fname)\n",
    "if not my_file.is_file():\n",
    "    url = 'https://hyhu.me/resources/' + fname\n",
    "    r = requests.get(url)\n",
    "\n",
    "    # Save to the current directory\n",
    "    with open(fname, 'wb') as f:\n",
    "        f.write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "來測試一下我們下載的結果，讀讀看第一行（第一份文件）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "John Blair & Co. is close to an agreement to sell its TV station advertising representation operation and program production unit to an investor group led by James H. Rosenfield, a former CBS Inc. executive, industry sources said. Industry sources put the value of the proposed acquisition at more than $100 million. John Blair was acquired last year by Reliance Capital Group Inc., which has been divesting itself of John Blair's major assets. John Blair represents about 130 local television stations in the placement of national and other advertising. Mr. Rosenfield stepped down as a senior executive vice president of CBS Broadcasting in December 1985 under a CBS early retirement program. Neither Mr. Rosenfield nor officials of John Blair could be reached for comment. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "\n",
    "raw_docs = []\n",
    "with gzip.open(fname, 'rt') as f:\n",
    "    for raw_doc in f:\n",
    "        raw_docs.append(raw_doc)\n",
    "\n",
    "print(len(raw_docs))\n",
    "print(raw_docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接著，我們開始預處理。我們可以先用NLTK的工具`word_tokenize`來*記號化*每份文件，接著使用`PorterStemmer`來*stem*和*小寫化(lowercase)*各文件，並且把這些正規化後的字彙加上Unique ID存進Python的*dict*資料型態，所有的字彙$M$有自己的字彙ID$[0..M-1]$。這過程可能需要幾分鐘，如果一直沒跑出來不要緊張，等它一下～"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents = 20000\n",
      "Number of unique terms = 103335\n",
      "Number of tokens = 9119196\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# processed_docs 儲存預處理過的文件列表\n",
    "processed_docs = []\n",
    "# vocab 儲存(term, term id)組合\n",
    "vocab = {}\n",
    "# total_tokens 儲存總共字數（不是字彙量，而是記號總量）\n",
    "total_tokens = 0\n",
    "\n",
    "# 使用PorterStemmer\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "for raw_doc in raw_docs:\n",
    "    \n",
    "    # norm_doc 儲存正規化後的文件\n",
    "    norm_doc = []\n",
    "    \n",
    "    # 使用word_tokenize\n",
    "    tokenized_document = nltk.word_tokenize(raw_doc)\n",
    "    for token in tokenized_document:\n",
    "        stemmed_token = stemmer.stem(token).lower()\n",
    "        norm_doc.append(stemmed_token)\n",
    "\n",
    "        total_tokens += 1\n",
    "        \n",
    "        # 將正規化後的字彙存進vocab (不重複存同樣字型的字彙)\n",
    "        if stemmed_token not in vocab:\n",
    "            vocab[stemmed_token] = len(vocab)\n",
    "            \n",
    "    processed_docs.append(norm_doc)\n",
    "\n",
    "print(\"Number of documents = {}\".format(len(processed_docs)))\n",
    "print(\"Number of unique terms = {}\".format(len(vocab)))\n",
    "print(\"Number of tokens = {}\".format(total_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接著，我們可以使用Python的`Counter`來計算每個文件的詞頻。我們將每個字彙當作key，它的詞頻當作value。最後我們將所有文件各自的`Counter`存進*doc_term_freqs*列表中。\n",
    "\n",
    "以一個Document為例：\n",
    "\n",
    ">the old night keeper keeps the keep in the town. in the big old house in the big old gown. The house in the town had the big old keep where the old night keeper never did sleep. The keeper keeps the keep in the night and keeps in the dark and sleeps in the light.\n",
    "\n",
    "在經過記號化和stemming之後，它的Counter應該長這樣：\n",
    "\n",
    "`\n",
    "Counter({'the': 14, 'in': 7, 'keep': 6, 'old': 5, '.': 4, 'night': 3, 'keeper': 3, 'big': 3, 'town': 2, 'hous': 2, 'sleep': 2, 'and': 2, 'gown': 1, 'had': 1, 'where': 1, 'never': 1, 'did': 1, 'dark': 1, 'light': 1})\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "Counter({'.': 6, 'john': 5, 'blair': 5, 'of': 5, 'to': 3, 'rosenfield': 3, ',': 3, 'a': 3, 'cb': 3, 'the': 3, 'an': 2, 'station': 2, 'advertis': 2, 'and': 2, 'program': 2, 'group': 2, 'by': 2, 'inc.': 2, 'execut': 2, 'industri': 2, 'sourc': 2, 'in': 2, 'mr.': 2, '&': 1, 'co.': 1, 'is': 1, 'close': 1, 'agreement': 1, 'sell': 1, 'it': 1, 'tv': 1, 'represent': 1, 'oper': 1, 'product': 1, 'unit': 1, 'investor': 1, 'led': 1, 'jame': 1, 'h.': 1, 'former': 1, 'said': 1, 'put': 1, 'valu': 1, 'propos': 1, 'acquisit': 1, 'at': 1, 'more': 1, 'than': 1, '$': 1, '100': 1, 'million': 1, 'wa': 1, 'acquir': 1, 'last': 1, 'year': 1, 'relianc': 1, 'capit': 1, 'which': 1, 'ha': 1, 'been': 1, 'divest': 1, 'itself': 1, \"'s\": 1, 'major': 1, 'asset': 1, 'repres': 1, 'about': 1, '130': 1, 'local': 1, 'televis': 1, 'placement': 1, 'nation': 1, 'other': 1, 'step': 1, 'down': 1, 'as': 1, 'senior': 1, 'vice': 1, 'presid': 1, 'broadcast': 1, 'decemb': 1, '1985': 1, 'under': 1, 'earli': 1, 'retir': 1, 'neither': 1, 'nor': 1, 'offici': 1, 'could': 1, 'be': 1, 'reach': 1, 'for': 1, 'comment': 1})\n",
      "Counter({'the': 9, 'of': 4, ',': 4, 'soviet': 3, 'to': 3, 'bank': 3, 'and': 3, 'said': 2, 'it': 2, 'an': 2, 'with': 2, 'institut': 2, 'in': 2, 'is': 2, 'aid': 2, '.': 2, 'banqu': 1, 'de': 1, \"l'union\": 1, 'europeen': 1, 'sign': 1, 'agreement': 1, 'two': 1, 'union': 1, 'that': 1, 'design': 1, 'format': 1, 'joint': 1, 'ventur': 1, 'under': 1, 'new': 1, 'rule': 1, 'french': 1, 'arm': 1, 'state-own': 1, 'credit': 1, 'industri': 1, '&': 1, 'commerci': 1, 'financi': 1, 'group': 1, 'will': 1, 'work': 1, 'state': 1, 'for': 1, 'foreign': 1, 'trade': 1, '``': 1, 'creat': 1, 'a': 1, 'bilater': 1, 'whose': 1, 'aim': 1, 'promot': 1, 'creation': 1, 'function': 1, 'financ': 1, 'mixed-capit': 1, 'compani': 1, 'u.s.s.r': 1, \"''\": 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# doc_term_freqs 儲存每個文件分別的字彙及詞頻Counter\n",
    "doc_term_freqs = []\n",
    "\n",
    "for norm_doc in processed_docs:\n",
    "    tfs = Counter()\n",
    "    # 計算詞頻\n",
    "    for token in norm_doc:\n",
    "        tfs[token] += 1\n",
    "    doc_term_freqs.append(tfs)\n",
    "\n",
    "print(len(doc_term_freqs))\n",
    "print(doc_term_freqs[0])\n",
    "print(doc_term_freqs[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 倒排索引"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再來就是我們的倒排索引，開發上我主要分成六個部分：\n",
    "\n",
    "1. 字彙表 `vocab` ，用以記錄字彙與其ID\n",
    "2. 每個文件的長度 `doc_len`\n",
    "3. `doc_ids` 是一個list，其中儲存了這個doc所包含的所有字彙的ID。is a list indexed by term IDs. For each term ID, it stores a list of document ids of all documents containing that term\n",
    "4. `doc_term_freqs` 是一個list，其中儲存了這個doc中相對應`doc_ids`的詞頻（就像Day 7中的第二個倒排索引）。每一個term ID都應該儲存著自己的文件-字彙詞頻列表(document term frequencies $f_{d,t}$)，這個列表說明了每個檔案 $d$ 中各個字彙 $t$ 分別出現的頻率 $f$ \n",
    "5. `doc_term_freqs` 記錄了各個詞出現在每個文件的詞頻， 而 `doc_freqs` 則記錄各個詞總共出現在多少個文件中。這會跟我們明天所要說的TF-IDF有關。文件頻率(Document Frequency) $f_t$ 的記法是，只要他曾經出現過，$f_t$ 就會+1。\n",
    "6. 最後我存了兩個數字 `total_num_docs` 和 `max_doc_len` ，他們分別記錄了總共處理的文件數量（應該要是兩萬份）以及單一文件最長的長度\n",
    "\n",
    "我所存的一些資料並不是為了之後計算TF-IDF用的，而是一些方便我們驗證開發正確性的統計數字。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "documents = 20000\n",
      "number of terms = 103335\n",
      "longest document length = 10576\n",
      "uncompressed space usage MiB = 58.219\n"
     ]
    }
   ],
   "source": [
    "class InvertedIndex:\n",
    "    def __init__(self, vocab, doc_term_freqs):\n",
    "        self.vocab = vocab\n",
    "        self.doc_len = [0] * len(doc_term_freqs)\n",
    "        self.doc_term_freqs = [[] for i in range(len(vocab))]\n",
    "        self.doc_ids = [[] for i in range(len(vocab))]\n",
    "        self.doc_freqs = [0] * len(vocab)\n",
    "        self.total_num_docs = 0\n",
    "        self.max_doc_len = 0\n",
    "        for docid, term_freqs in enumerate(doc_term_freqs):\n",
    "            doc_len = sum(term_freqs.values())\n",
    "            self.max_doc_len = max(doc_len, self.max_doc_len)\n",
    "            self.doc_len[docid] = doc_len\n",
    "            self.total_num_docs += 1\n",
    "            for term, freq in term_freqs.items():\n",
    "                term_id = vocab[term]\n",
    "                self.doc_ids[term_id].append(docid)\n",
    "                self.doc_term_freqs[term_id].append(freq)\n",
    "                self.doc_freqs[term_id] += 1\n",
    "\n",
    "    def num_terms(self):\n",
    "        return len(self.doc_ids)\n",
    "\n",
    "    def num_docs(self):\n",
    "        return self.total_num_docs\n",
    "\n",
    "    def docids(self, term):\n",
    "        term_id = self.vocab[term]\n",
    "        return self.doc_ids[term_id]\n",
    "\n",
    "    def freqs(self, term):\n",
    "        term_id = self.vocab[term]\n",
    "        return self.doc_term_freqs[term_id]\n",
    "\n",
    "    def f_t(self, term): \n",
    "        term_id = self.vocab[term]\n",
    "        return self.doc_freqs[term_id]\n",
    "\n",
    "    def space_in_bytes(self):\n",
    "        # 我們假設每個integer使用8 bytes\n",
    "        space_usage = 0\n",
    "        for doc_list in self.doc_ids:\n",
    "            space_usage += len(doc_list) * 8\n",
    "        for freq_list in self.doc_term_freqs:\n",
    "            space_usage += len(freq_list) * 8\n",
    "        return space_usage\n",
    "    \n",
    "\n",
    "invindex = InvertedIndex(vocab, doc_term_freqs)\n",
    "\n",
    "# print inverted index stats\n",
    "print(\"documents = {}\".format(invindex.num_docs()))\n",
    "print(\"number of terms = {}\".format(invindex.num_terms()))\n",
    "print(\"longest document length = {}\".format(invindex.max_doc_len))\n",
    "print(\"uncompressed space usage MiB = {:.3f}\".format(invindex.space_in_bytes() / (1024.0 * 1024.0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 10: TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們現在根據Day 9中開發的 `InvertedIndex` 類別來計算文件和查詢 $Q$ 之間的TF-IDF相似度。\n",
    "\n",
    "在這裡，我們使用簡易版的TF-IDF相似度計算公式：\n",
    "\n",
    "\\begin{equation*}\n",
    "Score(Q,d) = \\frac{1}{\\sqrt{|d|}} \\times \\sum_{i=1}^q \\log(1 + f_{d,t}) * \\log( \\frac{N}{f_t} ) \n",
    "\\end{equation*}\n",
    "\n",
    "其中 $Q$ 指的是我們的查詢，包含了多個查詢字 $q$ 。 $\\sqrt{|d|}$ 說明了文件的長度， $f_{d,t}$ 是字詞 $t$ 在文件 $d$ 中的出現頻率， $N$ 是指在文件集中的文件總數， 最後 $f_t$ 為包含了字詞 $t$ 的文件數量。這些資料都是我們在 `InvertedIndex` 類別中預先存好的。\n",
    "\n",
    "我們的 `query_tfidf` function中輸入一句查詢以及一個索引類別，最後回傳 top-$k$ TF-IDF分數最高的文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "排名  1 DOCID     1084 SCORE 1.210 內容 Unemployment in South Korea fell to 3.8% of the labor force last year from \n",
      "排名  2 DOCID      905 SCORE 1.127 內容 Seasonally adjusted industrial production in South Korea increased nearly 2\n",
      "排名  3 DOCID     5612 SCORE 1.056 內容 Consumer prices in South Korea rose 2.4% in April from a year-earlier and 0\n",
      "排名  4 DOCID     9126 SCORE 0.998 內容 Foreign investment in South Korea totaled $278 million in 1987's first quar\n",
      "排名  5 DOCID    17960 SCORE 0.936 內容 Henley Group Inc. said its M.W. Kellogg Co. unit received a contract to des\n",
      "排名  6 DOCID     1760 SCORE 0.926 內容 Consumer prices in South Korea rose 1% in February from a year earlier, the\n",
      "排名  7 DOCID     4132 SCORE 0.926 內容 South Korea's trade deficit with Japan grew to a record $629 million in Apr\n",
      "排名  8 DOCID    17826 SCORE 0.923 內容 South Korea revised its 1986 current-account surplus to $5 billion from the\n",
      "排名  9 DOCID    15803 SCORE 0.911 內容 South Korea's 1986 trade surplus with the U.S. was revised upward to $7.41 \n",
      "排名 10 DOCID    10664 SCORE 0.889 內容 South Korea's economy, aided by brisk exports, grew an inflation-adjusted 1\n"
     ]
    }
   ],
   "source": [
    "from math import log, sqrt\n",
    "\n",
    "# 給定一個查詢(String)和一個索引(Class)，回傳k個文件\n",
    "def query_tfidf(query, index, k=10):\n",
    "    \n",
    "    # scores 儲存了docID和他們的TF-IDF分數\n",
    "    scores = Counter()\n",
    "    \n",
    "    N = index.num_docs()\n",
    "    \n",
    "    for term in query:\n",
    "        i = 0\n",
    "        f_t = index.f_t(term)\n",
    "        for docid in index.docids(term):\n",
    "            # f_(d,t)\n",
    "            f_d_t = index.freqs(term)[i]\n",
    "            d = index.doc_len[docid]\n",
    "            tfidf_cal = log(1+f_d_t) * log(N/f_t) / sqrt(d)\n",
    "            scores[docid] += tfidf_cal\n",
    "            i += 1\n",
    "    \n",
    "    return scores.most_common(k)\n",
    "\n",
    "# 查詢語句\n",
    "query = \"south korea production\"\n",
    "# 預處理查詢，為了讓查詢跟索引內容相同\n",
    "stemmed_query = nltk.stem.PorterStemmer().stem(query).split()\n",
    "results = query_tfidf(stemmed_query, invindex)\n",
    "for rank, res in enumerate(results):\n",
    "    # e.g 排名 1 DOCID 176 SCORE 0.426 內容 South Korea rose 1% in February from a year earlier, the\n",
    "    print(\"排名 {:2d} DOCID {:8d} SCORE {:.3f} 內容 {:}\".format(rank+1,res[0],res[1],raw_docs[res[0]][:75]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 12: Vbyte 壓縮"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今天我們要來實作倒排索引的空間壓縮。這裡我們會利用昨天文中介紹的VByte壓縮法壓縮倒排索引中的文件ID `doc_ids` 以及文件-詞頻列表 `doc_term_freqs` 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在昨天的文中，我們有附上Vbyte壓縮和解壓縮的演算法，我們將開發成以下兩個方法：\n",
    "\n",
    "- `vbyte_encode(num)`：接受一個數字，回傳相對該數字的Vbyte壓縮。\n",
    "- `vbyte_decode(input_bytes, idx)`：接受一組Vbyte壓縮（可能是多個位元組，根據Continuation Bit來決定有幾個bytes），回傳解壓縮後的數字。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vbyte_encode(num):\n",
    "\n",
    "    # out_bytes 儲存轉換成Vbyte壓縮後的格式\n",
    "    out_bytes = []\n",
    "    \n",
    "    while num >= 128:\n",
    "        out_bytes.append(int(num) % 128)\n",
    "        num /= 128\n",
    "        \n",
    "    out_bytes.append(int(num) + 128)\n",
    "    \n",
    "    return out_bytes\n",
    "\n",
    "\n",
    "def vbyte_decode(input_bytes, idx):\n",
    "    \n",
    "    x = 0 # 儲存解壓縮後的數字\n",
    "    s = 0\n",
    "    consumed = 0 # 記錄花了多少位元組來解壓這個數字\n",
    "    \n",
    "    while input_bytes[idx + consumed] < 128:\n",
    "        x ^= (input_bytes[idx + consumed] << s)\n",
    "        s += 7\n",
    "        consumed += 1\n",
    "    \n",
    "    x ^= ((input_bytes[idx + consumed]-128) << s)\n",
    "    consumed += 1\n",
    "    \n",
    "    return x, consumed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "單元測試壓縮和解壓縮過程正確性："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num in range(0, 123456):\n",
    "    vb = vbyte_encode(num)\n",
    "    dec, decoded_bytes = vbyte_decode(vb, 0)\n",
    "    assert(num == dec)\n",
    "    assert(decoded_bytes == len(vb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正確地開發了VByte壓縮和解壓縮之後，我們來修正原本的 `InvertedIndex` 類別以支援VByte壓縮。需要注意的是， `doc_ids` 的部份是要壓縮文件ID之間的間隔而不是文件ID本身。我還寫了一個輔助方法 `decompress_list` 來幫助我們更簡單地將列表解壓縮。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "documents = 20000\n",
      "unique terms = 103335\n",
      "longest document = 10576\n",
      "compressed space usage MiB = 7.823\n"
     ]
    }
   ],
   "source": [
    "def decompress_list(input_bytes, gapped_encoded):\n",
    "    res = []\n",
    "    prev = 0\n",
    "    idx = 0\n",
    "    while idx < len(input_bytes):\n",
    "        dec_num, consumed_bytes = vbyte_decode(input_bytes, idx)\n",
    "        idx += consumed_bytes\n",
    "        num = dec_num + prev\n",
    "        res.append(num)\n",
    "        if gapped_encoded:\n",
    "            prev = num\n",
    "    return res\n",
    "\n",
    "class CompressedInvertedIndex:\n",
    "    def __init__(self, vocab, doc_term_freqs):\n",
    "        self.vocab = vocab\n",
    "        self.doc_len = [0] * len(doc_term_freqs)\n",
    "        self.doc_term_freqs = [[] for i in range(len(vocab))]\n",
    "        self.doc_ids = [[] for i in range(len(vocab))]\n",
    "        self.doc_freqs = [0] * len(vocab)\n",
    "        self.total_num_docs = 0\n",
    "        self.max_doc_len = 0\n",
    "        for docid, term_freqs in enumerate(doc_term_freqs):\n",
    "            doc_len = sum(term_freqs.values())\n",
    "            self.max_doc_len = max(doc_len, self.max_doc_len)\n",
    "            self.doc_len[docid] = doc_len\n",
    "            self.total_num_docs += 1\n",
    "            for term, freq in term_freqs.items():\n",
    "                term_id = vocab[term]\n",
    "                self.doc_ids[term_id].append(docid)\n",
    "                self.doc_term_freqs[term_id].append(freq)\n",
    "                self.doc_freqs[term_id] += 1\n",
    "        \n",
    "        # 壓縮文件ID之間的間隔\n",
    "        for i in range(len(self.doc_ids)):\n",
    "            last_docid = self.doc_ids[i][0]\n",
    "            for j in range(len(self.doc_ids[i])):\n",
    "                if j != 0:\n",
    "                    ori_docid = self.doc_ids[i][j]\n",
    "                    self.doc_ids[i][j] = vbyte_encode(self.doc_ids[i][j] - last_docid)\n",
    "                    last_docid = ori_docid\n",
    "                else:\n",
    "                    self.doc_ids[i][0] = vbyte_encode(last_docid)\n",
    "            self.doc_ids[i] = sum(self.doc_ids[i], [])\n",
    "            \n",
    "        # 根據詞頻壓縮\n",
    "        for i in range(len(self.doc_term_freqs)):\n",
    "            for j in range(len(self.doc_term_freqs[i])):\n",
    "                self.doc_term_freqs[i][j] = vbyte_encode(self.doc_term_freqs[i][j])\n",
    "            self.doc_term_freqs[i] = sum(self.doc_term_freqs[i], [])\n",
    "    \n",
    "    def num_terms(self):\n",
    "        return len(self.doc_ids)\n",
    "\n",
    "    def num_docs(self):\n",
    "        return self.total_num_docs\n",
    "\n",
    "    def docids(self, term):\n",
    "        term_id = self.vocab[term]\n",
    "        # 解壓縮\n",
    "        return decompress_list(self.doc_ids[term_id], True)\n",
    "\n",
    "    def freqs(self, term):\n",
    "        term_id = self.vocab[term]\n",
    "        # 解壓縮\n",
    "        return decompress_list(self.doc_term_freqs[term_id], False)\n",
    "\n",
    "    def f_t(self, term):\n",
    "        term_id = self.vocab[term]\n",
    "        return self.doc_freqs[term_id]\n",
    "\n",
    "    def space_in_bytes(self):\n",
    "        # 這裡現在假設數字都是位元組型態\n",
    "        space_usage = 0\n",
    "        for doc_list in self.doc_ids:\n",
    "            space_usage += len(doc_list)\n",
    "        for freq_list in self.doc_term_freqs:\n",
    "            space_usage += len(freq_list)\n",
    "        return space_usage\n",
    "\n",
    "\n",
    "compressed_index = CompressedInvertedIndex(vocab, doc_term_freqs)\n",
    "\n",
    "print(\"documents = {}\".format(compressed_index.num_docs()))\n",
    "print(\"unique terms = {}\".format(compressed_index.num_terms()))\n",
    "print(\"longest document = {}\".format(compressed_index.max_doc_len))\n",
    "print(\"compressed space usage MiB = {:.3f}\".format(compressed_index.space_in_bytes() / (1024.0 * 1024.0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在資料不變的情況下，我們的空間使用已經從58.187MiB降到了7.818MiB。\n",
    "\n",
    "最後，我們來測試原本的倒排索引及VByte壓縮後的倒排索引結果有沒有改變（理論上結果該相同）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "排名  1 DOCID     1084 SCORE 1.210 內容 Unemployment in South Korea fell to 3.8% of the labor force last year from \n",
      "排名  2 DOCID      905 SCORE 1.127 內容 Seasonally adjusted industrial production in South Korea increased nearly 2\n",
      "排名  3 DOCID     5612 SCORE 1.056 內容 Consumer prices in South Korea rose 2.4% in April from a year-earlier and 0\n",
      "排名  4 DOCID     9126 SCORE 0.998 內容 Foreign investment in South Korea totaled $278 million in 1987's first quar\n",
      "排名  5 DOCID    17960 SCORE 0.936 內容 Henley Group Inc. said its M.W. Kellogg Co. unit received a contract to des\n",
      "排名  6 DOCID     1760 SCORE 0.926 內容 Consumer prices in South Korea rose 1% in February from a year earlier, the\n",
      "排名  7 DOCID     4132 SCORE 0.926 內容 South Korea's trade deficit with Japan grew to a record $629 million in Apr\n",
      "排名  8 DOCID    17826 SCORE 0.923 內容 South Korea revised its 1986 current-account surplus to $5 billion from the\n",
      "排名  9 DOCID    15803 SCORE 0.911 內容 South Korea's 1986 trade surplus with the U.S. was revised upward to $7.41 \n",
      "排名 10 DOCID    10664 SCORE 0.889 內容 South Korea's economy, aided by brisk exports, grew an inflation-adjusted 1\n"
     ]
    }
   ],
   "source": [
    "# 確認是否和先前結果相同\n",
    "query = \"south korea production\"\n",
    "stemmed_query = nltk.stem.PorterStemmer().stem(query).split()\n",
    "comp_results = query_tfidf(stemmed_query, compressed_index)\n",
    "for rank, res in enumerate(comp_results):\n",
    "    print(\"排名 {:2d} DOCID {:8d} SCORE {:.3f} 內容 {:}\".format(rank+1,res[0],res[1],raw_docs[res[0]][:75]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不論DocID排名或是TF-IDF分數都沒有改變，壓縮結果正確。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
