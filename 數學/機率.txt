< Probability >
    Distribution å…¶å¯¦å°±æ˜¯çœ‹ æ¯å€‹çµæœçš„ Probability
P(x,y) : x,y åŒæ™‚ç™¼ç”Ÿçš„æ©Ÿç‡          (Joint Probability)
P(x|y) : ç•¶ y å·²ç¶“ç™¼ç”Ÿ, x ç™¼ç”Ÿçš„æ©Ÿç‡ (Conditional Probability)
    Probability of x given that y
    P(y) : Prior Distribution
P(x,y | z) : ç•¶ z å·²ç¶“ç™¼ç”Ÿ, x,y åŒæ™‚ç™¼ç”Ÿçš„æ©Ÿç‡
P(x;y) : æˆ‘å·²ç¶“ç¢ºå®š y (å›ºå®š æˆ– 100%æœƒç™¼ç”Ÿ), x ç™¼ç”Ÿçš„æ©Ÿç‡


Sum Rule, Marginalization : 
    P(x) = sum(P(x,y) for all y)
    P(x|z) = sum(P(x,y|z) for all y)
        è¦å‰‡ : æŠŠæŸå€‹è®Šæ•¸çš„å¯èƒ½æ€§åŠ èµ·ä¾†ï¼Œå°±å¯ä»¥å»æ‰é‚£å€‹è®Šæ•¸

    Sum Rule : emphasise - we what to cal P(x) by adding all possible y
    Marginalization : emphasise - we don't care about y, so we try to remove y
        é€™å€‹ y å«åš Nuisance Variable : å¤šé¤˜è®Šæ•¸ã€å¹²æ“¾è®Šæ•¸

Bayes' Rule
    P(Hypothesis|Evidence) / P(H|E) : the posibility of Hypothesis given Evidence
        = P(E|H) * P(H) / P(E) 
        = P(E,H) / P(E)
            P(H) : Prior Distribution   (å…ˆé©— : å› ç‚ºæ˜¯é‚„æ²’æœ‰ä»»ä½•ç·šç´¢ å¾—å‡ºçš„æ©Ÿç‡)
            P(H|E) : Posterior Distribution (å¾Œé©— : å› ç‚ºæ˜¯æœ‰äº†æ–°ç·šç´¢ å¾—å‡ºçš„æ©Ÿç‡)
            P(E|H) : Likelihood
            P(E) : Evidence
        åæ­£å°±è¨˜å¾— æ¢ä»¶ ä¸å¯èƒ½åœ¨åŒä¸€é‚Š : P(A|?) ä¸å¯èƒ½å†ä¹˜P(A) æ‰€ä»¥P(A)ä¸€å®šåœ¨ä¸‹é¢

Chain Rule (expand from Bayes' Rule)
    P(x,y|z) = P(x|y,z) * P(y|z)

< likelihood >
L(y|x) = P(x|y)

likelihood function
    L(Î¸) = L(Î¸|x) = L(Î¸;x) = p(x|Î¸) = MUL( p(xi | Î¸) for all i )
        Î¸ åŒ…å«äº†æ‰€æœ‰çš„æ¬Šé‡ï¼ˆweightsï¼‰èˆ‡åå·®ï¼ˆbiasï¼‰
log-likelihood function
    log L(Î¸) = â„“(Î¸) 
        = log MUL {i=1}^n p(xi | Î¸)
        = sum( log p(xi | Î¸) for all i )

< odds >
odds = p / (1-p)
    æœƒç™¼ç”Ÿçš„æ©Ÿç‡ èˆ‡ ä¸ç™¼ç”Ÿçš„æ©Ÿç‡ çš„ æ¯”ä¾‹

log odds
    å‡è¨­æŸäº‹ä»¶xç™¼ç”Ÿçš„æ©Ÿç‡æ˜¯ p, p åªèƒ½åœ¨ 0 åˆ° 1 ä¹‹é–“
    ç•¶æˆ‘å° odd å– log æ™‚
        log( p(x) / (1-p(x)) )
            p(x) > 0.5 æ™‚ log odds ç‚ºæ­£
            p(x) < 0.5 æ™‚ log odds ç‚ºè² 
    å› æ­¤å¯ä»¥ æŠŠé€™å€‹æ¯”ä¾‹è½‰æˆä¸€å€‹ç·šæ€§ã€å¯åŠ ã€ç„¡ç•Œçš„å°ºåº¦

< Expectation >
    Discrete distribution
        E[X] = sum( p(xi)*f(xi) for all i )
            f(xi) å¯ä»¥æ˜¯ä¸€å€‹ function, æˆ–æœ¬èº« xi æ•¸å€¼ (f(xi) = xi)
    continuous
        E[X] = âˆ« x*f(x) dx
            f(x) is PDF


discriminative model VS generative model
    Y is output(labels), X is input(feature)
    discriminative model :
        learning P(Y|X)
            since model is learning the relationship between X and Y "directly"
    generative model  
        learning P(X,Y)
            usually learning P(X|Y) and P(Y), since P(X,Y) = P(X|Y)*P(Y)
                P(Y) is Prior Distribution, represent the distribution of Y
                P(X|Y) is Likelihood, represent if knowing the output is Y what should X looks like




-- < æ©Ÿç‡åˆ†å¸ƒ Probability Distribution > ----------------------------------------------------------
    Probability Distribution çš„ç‰¹é»æ˜¯ ç¸½åˆç‚º0

Discrete distribution
    Probability Mass Function (PMF)
    å° PMF ç›¸åŠ  å¯ä»¥å¾—å‡º CDF
        
continuous distribution
    å› ç‚ºæ˜¯ continuous æ‰€ä»¥ æˆ‘æ²’è¾¦æ³•èªªç‰¹å®šä¸€å€‹é» çš„æ©Ÿç‡æ˜¯å¤šå°‘
    ä½†å¯ä»¥èªªæŸå€‹æ•¸å­—ä»¥ä¸‹(æˆ–æŸå€‹å€é–“)çš„æ©Ÿç‡æ˜¯å¤šå°‘
    Probability Density Function (PDF)
        Gaussian distributionï¼ˆé«˜æ–¯åˆ†ä½ˆï¼‰èˆ‡ Normal distributionï¼ˆå¸¸æ…‹åˆ†ä½ˆ/æ­£æ…‹åˆ†ä½ˆ) æ˜¯ç›¸åŒçš„
    å° PDF åšç©åˆ† å¯ä»¥å¾—å‡º CDF

Cumulative Distribution Function (CDF) ç´¯ç©åˆ†ä½ˆå‡½æ•¸

ç‰¹æ®Šçš„ distribution åç¨±
Bernoulli distribution
    åœ¨ 1 æ¬¡ç¨ç«‹è©¦é©—ä¸­ï¼Œé€™ä¸€æ¬¡åªæœƒå‡ºç¾ 0/1 å…©ç¨®çµæœ
        P(Y = 1) = p
        P(Y = 0) = 1 âˆ’ p
    å¦‚æœæˆ‘æƒ³è¦ç”¨ä¸€å€‹å¼å­è¡¨ç¤º P(Y)
        P(Y) = p^y*(1-p)^(1-y)
        P(Y = 1) = p     = p^y*(1-p)^(1-y), y=1 = p^1*(1-p)^(0) = p
        P(Y = 0) = 1 âˆ’ p = p^y*(1-p)^(1-y), y=0 = p^0*(1-p)^(1) = 1-p
Binomial distribution : 
    åœ¨ n æ¬¡ç¨ç«‹è©¦é©—ä¸­ï¼Œæ¯ä¸€æ¬¡åªæœƒå‡ºç¾ 0/1 å…©ç¨®çµæœ
    æ©«è»¸è¡¨ç¤º å†næ¬¡çš„è©¦é©—ä¸­ç¸½å…±æˆåŠŸxæ¬¡ 
    ç¸±è»¸è¡¨ç¤º æˆåŠŸxæ¬¡ çš„æ©Ÿç‡
    (Binomial(n, p) å°±æ˜¯ n æ¬¡ç¨ç«‹çš„ Bernoulli åŠ èµ·ä¾†)

normal distribution (Gaussian distribution) : å¸¸æ…‹åˆ†å¸ƒ
    ç¬¦è™Ÿ : ğ’©(Î¼, ÏƒÂ²)
        Î¼ æ˜¯å¹³å‡æ•¸ mean, ä¹Ÿå°±æ˜¯ä¸­å¿ƒä½ç½®
    Gaussian function (Gaussian Equation) æŒ‡çš„æ˜¯ä¸€å€‹å‡½æ•¸å½¢ç‹€
        f(x) = e^(-(x-Î¼)Â² / (2ÏƒÂ²))
    Gaussian distribution function <Gaussian distribution function.png>
        åŠ å…¥ normalization term : (1/(2Ï€Ïƒ^2)^(1/2))
    p(x) = (1/(2Ï€Ïƒ^2)^(1/2))*Gaussian function
         = (1/(2Ï€Ïƒ^2)^(1/2))*e^(-(x-Î¼)Â² / (2ÏƒÂ²))
         å¦å¤–ä¸€ç¨®å½¢å¼ (æŠŠ Ïƒ è·Ÿ 1/2 æå–åˆ°å¤–é¢)
         = (1/Ïƒ(2Ï€)^(1/2)) * e^(1/2)*(-(x-Î¼)Â² / ÏƒÂ² )
            x-Î¼ å¯ä»¥æ›¿æ›æˆ error
    
zero mean normal distribution
    ç¬¦è™Ÿ : ğ’©(0, ÏƒÂ²)
    ä»£è¡¨ åˆ†å¸ƒçš„ä¸­å¿ƒåœ¨ 0

Naive Bayes
    Why Naive 
        In Bayes, all features are assumed to be independent of one another.
        However, even this condition is usually incorrect in real life
        the result still turns out to be good.
    æ­¥é©Ÿ <ä»¥ binary spam ç•¶ä½œç¯„ä¾‹>
        è¨ˆç®— prior + easier to cal
            åœ¨æ²’æœ‰çœ‹åˆ°ä»»ä½•å­—ä¹‹å‰ï¼Œæ˜¯ spam çš„æ©Ÿç‡
            ori      : prior = P(spam)/P(normal)
            logprior : log( amount of spam / amount of normal )
        è¨ˆç®— likelihood + Laplace smoothing + easier to cal
            Laplace smoothing : é¿å…æŸå€‹å­—æ²’å‡ºç¾å°è‡´æ©Ÿç‡ç‚º 0
            easier to cal : log
            ori :   loglikelihood[word] =        P(word_i in spam | spam) /   P(word_i in normal | normal) 
            adjust: loglikelihood[word] = log( L_P(word_i in spam | spam) / L_P(word_i in normal | normal) )
                L_P(word_i in spam | spam) = word_i åœ¨ spam å¥å­ä¸­å‡ºç¾å¹¾æ¬¡"+1" / spam ç¸½å…±æœ‰å¹¾å€‹å–®å­— "+V"
                    Laplace smoothing : +1 / +V
                    Multinomial Naive Bayes : æœƒé‡è¤‡è¨ˆç®—é‡è¤‡çš„å–®å­—
                    Bernoulli Naive Bayes   : ä¸€å€‹å¥å­ä¸­é‡è¤‡å‡ºç¾çš„å–®å­—åªç®—ä¸€æ¬¡
                V : æ‰€æœ‰ä¸é‡è¤‡å–®å­—çš„ç¸½æ•¸
                word_i in spam : word_i åœ¨ spam è£¡é¢å‡ºç¾éå¤šå°‘æ¬¡
        é æ¸¬æ–°æ–‡ä»¶
            pred = logprior + sum(loglikelihood[word] for word in "free win money")
            threshold = 0 # å¦‚æœå¸Œæœ›æ›´åš´æ ¼å¯ä»¥å¾€ä¸Šèª¿æ•´
            if pred > threshold :
                is spam

