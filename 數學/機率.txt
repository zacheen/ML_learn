< Probability >
    Distribution å…¶å¯¦å°±æ˜¯çœ‹ æ¯å€‹çµæœçš„ Probability
P(x,y) : x,y åŒæ™‚ç™¼ç”Ÿçš„æ©Ÿç‡          (Joint Probability)
P(x|y) : ç•¶ y å·²ç¶“ç™¼ç”Ÿ, x ç™¼ç”Ÿçš„æ©Ÿç‡ (Conditional Probability)
    Probability of x given that y
    P(y) : Prior Distribution
P(x,y | z) : ç•¶ z å·²ç¶“ç™¼ç”Ÿ, x,y åŒæ™‚ç™¼ç”Ÿçš„æ©Ÿç‡


Sum Rule, Marginalization : 
    P(x) = sum(P(x,y) for all y)
    P(x|z) = sum(P(x,y|z) for all y)
        è¦å‰‡ : æŠŠæŸå€‹è®Šæ•¸çš„å¯èƒ½æ€§åŠ èµ·ä¾†ï¼Œå°±å¯ä»¥å»æ‰é‚£å€‹è®Šæ•¸

    Sum Rule : emphasise - we what to cal P(x) by adding all possible y
    Marginalization : emphasise - we don't care about y, so we try to remove y
        é€™å€‹ y å«åš Nuisance Variable : å¤šé¤˜è®Šæ•¸ã€å¹²æ“¾è®Šæ•¸

Bayes' Rule
    P(x|y) = P(y|x) * P(x) / P(y) = P(x,y) / P(y)
        P(x) : Prior Distribution
        P(y|x) : Likelihood
        P(x|y) : Posterior Distribution
            Prior èˆ‡ likelihood çµåˆ å½¢æˆ posteriorã€‚
Chain Rule (expand from Bayes' Rule)
    P(x,y|z) = P(x|y,z) * P(y|z)


< likelihood >
L(y|x) = P(x|y)

< odds >
odds = p / (1-p)
    æœƒç™¼ç”Ÿçš„æ©Ÿç‡ èˆ‡ ä¸ç™¼ç”Ÿçš„æ©Ÿç‡ çš„ æ¯”ä¾‹

log odds
    å‡è¨­æŸäº‹ä»¶xç™¼ç”Ÿçš„æ©Ÿç‡æ˜¯ p, p åªèƒ½åœ¨ 0 åˆ° 1 ä¹‹é–“
    ç•¶æˆ‘å° odd å– log æ™‚
        log( p(x) / (1-p(x)) )
            p(x) > 0.5 æ™‚ log odds ç‚ºæ­£
            p(x) < 0.5 æ™‚ log odds ç‚ºè² 
    å› æ­¤å¯ä»¥ æŠŠé€™å€‹æ¯”ä¾‹è½‰æˆä¸€å€‹ç·šæ€§ã€å¯åŠ ã€ç„¡ç•Œçš„å°ºåº¦


discriminative model VS generative model
    Y is output(labels), X is input(feature)
    discriminative model :
        learning P(Y|X)
            since model is learning the relationship between X and Y "directly"
    generative model  
        learning P(X,Y) 
            usually learning P(X|Y) and P(Y), since P(X,Y) = P(X|Y)*P(Y)
                P(Y) is Prior Distribution, represent the distribution of Y
                P(X|Y) is Likelihood, represent if knowing the output is Y what should X looks like


Naive Bayes??

-- < æ©Ÿç‡åˆ†å¸ƒ > ----------------------------------------------------------
Discrete distribution
    Probability Mass Function (PMF)
    å° PMF ç›¸åŠ  å¯ä»¥å¾—å‡º CDF
        
continuous distribution
    å› ç‚ºæ˜¯ continuous æ‰€ä»¥ æˆ‘æ²’è¾¦æ³•èªªç‰¹å®šä¸€å€‹é» çš„æ©Ÿç‡æ˜¯å¤šå°‘
    ä½†å¯ä»¥èªªæŸå€‹æ•¸å­—ä»¥ä¸‹(æˆ–æŸå€‹å€é–“)çš„æ©Ÿç‡æ˜¯å¤šå°‘
    Probability Density Function (PDF)
        Gaussian distributionï¼ˆé«˜æ–¯åˆ†ä½ˆï¼‰èˆ‡ Normal distributionï¼ˆå¸¸æ…‹åˆ†ä½ˆ/æ­£æ…‹åˆ†ä½ˆ) æ˜¯ç›¸åŒçš„
    å° PDF åšç©åˆ† å¯ä»¥å¾—å‡º CDF

Cumulative Distribution Function (CDF) ç´¯ç©åˆ†ä½ˆå‡½æ•¸

ç‰¹æ®Šçš„ distribution åç¨±
Bernoulli distribution
    åœ¨ 1 æ¬¡ç¨ç«‹è©¦é©—ä¸­ï¼Œé€™ä¸€æ¬¡åªæœƒå‡ºç¾ 0/1 å…©ç¨®çµæœ
        P(Y = 1) = p
        P(Y = 0) = 1 âˆ’ p
Binomial distribution : 
    åœ¨ n æ¬¡ç¨ç«‹è©¦é©—ä¸­ï¼Œæ¯ä¸€æ¬¡åªæœƒå‡ºç¾ 0/1 å…©ç¨®çµæœ
    æ©«è»¸è¡¨ç¤º å†næ¬¡çš„è©¦é©—ä¸­ç¸½å…±æˆåŠŸxæ¬¡ 
    ç¸±è»¸è¡¨ç¤º æˆåŠŸxæ¬¡ çš„æ©Ÿç‡
    (Binomial(n, p) å°±æ˜¯ n æ¬¡ç¨ç«‹çš„ Bernoulli åŠ èµ·ä¾†)

normal distribution (Gaussian distribution) : å¸¸æ…‹åˆ†å¸ƒ
    ç¬¦è™Ÿ : ğ’©(Î¼, ÏƒÂ²)
        Î¼ æ˜¯å¹³å‡æ•¸ mean, ä¹Ÿå°±æ˜¯ä¸­å¿ƒä½ç½®
    Gaussian function (Gaussian Equation) æŒ‡çš„æ˜¯ä¸€å€‹å‡½æ•¸å½¢ç‹€
        f(x) = e^(-(x-Î¼)Â² / (2ÏƒÂ²))
    Gaussian distribution function <Gaussian distribution function.png>
        åŠ å…¥ normalization term : (1/(2Ï€Ïƒ^2)^(1/2))
    p(x) = (1/(2Ï€Ïƒ^2)^(1/2))*Gaussian function
         = (1/(2Ï€Ïƒ^2)^(1/2))*e^(-(x-Î¼)Â² / (2ÏƒÂ²))
         å¦å¤–ä¸€ç¨®å½¢å¼ (æŠŠ Ïƒ è·Ÿ 1/2 æå–åˆ°å¤–é¢)
         = (1/Ïƒ(2Ï€)^(1/2)) * e^(1/2)*(-(x-Î¼)Â² / ÏƒÂ² )
            x-Î¼ å¯ä»¥æ›¿æ›æˆ error
    
zero mean normal distribution
    ç¬¦è™Ÿ : ğ’©(0, ÏƒÂ²)
    ä»£è¡¨ åˆ†å¸ƒçš„ä¸­å¿ƒåœ¨ 0

p(x)=2Ï€Ïƒ2
â€‹1â€‹exp(âˆ’2Ïƒ2(xâˆ’Î¼)2â€‹)





