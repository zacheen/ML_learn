binomial 二項式

regression that can regularization
    by adding limitation to B to prevent B explosion
        限制 ||B||^1 or ||B||^2 是為了防止 B 太大，因為 linear regression 中 係數B的大小直接反映了模型對特徵的敏感度
        (This problem actually only happen in linear models (include linear regression and logistic regression))
    ridge regression 
        Ridge 限制的是係數的平方和（L2 範數）
        式子 : min(||y-xB||^2) s.t. ||B||^2 <= c^2
        透過 Lagrange 轉換 : min(||y-xB||^2) + λ(c^2 - ||B||^2)
            ||B||^2 其實也是 L2 norm : sum(B^2 for all B)
    lasso regression (Least Absolute Shrinkage and Selection Operator)
        Lasso 限制的是係數的絕對值之和（L1 範數）
        式子 : min(||y-xB||^2) s.t. ||B||^1 <= c
        透過 Lagrange 轉換 : min(||y-xB||^2) + λ(c - ||B||^1)
            ||B||^1 其實也是 L1 norm : sum(|B| for all B)
        在幾何圖上 這個 boundary 會有菱有角，這些頂點很容易是最佳解 : 所以才能達成「特徵選取」
        這些最佳解會直接讓 某個沒有貢獻的特徵直接歸零
    c 的影響
        如果c太大，dominant 是 λ(c^2 - ||B||^2) : 無法抑制 B 變很大
        如果c太小，dominant 是 min(||y-xB||^2)  : 那就跟原式子一樣，沒有 generalizing 的效果
    combine ridge and lasso : Elastic Net
        Lenet(B') = (||y-xB||^2)/2n + λ( ((1-a)/2)*||B||^2 + a*||B||^1)
            這裡的 /2n, /2 都是為了方便 微分之後可以抵消

convex function 凸函數(像碗)   Ex: -log(x), x^2
    f((a+b)/2) "<=" (f(a) + f(b))/2, for any point a,b 
concave function 凹函數(像山丘) Ex: log(x)
    f((a+b)/2) ">=" (f(a) + f(b))/2, for any point a,b
Jensen's Inequality (expand the convex/concave function)
    (這裡使用 Convex function 來舉例 , concave function 一樣的道理)
    original
        for all a,b f(a+b)/2 <= (f(a) + f(b))/2 for convex function
    change (1/2) to λ
        for all a,b f(a+b)/λ <= (λf(a) + (1-λ)f(b))/2 for convex function
    expand it from 2 number to multiple numbers
        f(sum(λi*xi)) <= sum(λi*f(xi))
            λi : 介於 0~1 之間對應 xi 的機率 且 總和為1
    轉成數學符號
        f( E[X] ) <= E[ f(X) ]

衡量兩個變數之間"線性關係"
    Covariance 協方差
        Cov(X, Y) = E[(X − μx)(Y − μy)]
            = 1/(n-1)*∑(Xi-X_mean)(Yi-Y_mean)
        代表 x, y 之間 方向是否一致 (正 或 負)
        當我們把全部的點正規化(減去平均) 會變成很簡單的式子
            = 1/(n-1)*∑(Xi)(Yi)
        正規化(中心化)後的協方差矩陣
            Σ = (1 / (n − 1)) XᵀX
                X 是 data 矩陣
    Correlation
        Corr(X, Y) = Cov(X, Y) / (Std(X) * Std(Y))
            -1 ~ 1
        代表 x, y 之間 方向是否一致 + 資料點相對於一條直線的「集中程度」(|Correlation數值|)
     

