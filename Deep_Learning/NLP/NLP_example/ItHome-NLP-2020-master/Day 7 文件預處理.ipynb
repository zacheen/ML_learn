{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 這裡面就只是把網站爬下來的東西 \n",
    "# 1. 擷取文字段落(並不是通用的)\n",
    "# 正規化 (Normalize)\n",
    "#     2. 架構分割(Structure Segment) : 文章分割成句子\n",
    "#     3. 記號化(Tokenize)            : 句子分割成一個一個的單字 list\n",
    "#     4. 文字正規化 (Normalize)      : 將單字正規化成固定格式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 7: 文件預處理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，我從我的一個網站中截下一段HTML："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\n",
    " <body>\n",
    "    <!-- JavaScript plugins (requires jQuery) -->\n",
    "    <script src=\"http://code.jquery.com/jquery.js\"></script>\n",
    "    <!-- Include all compiled plugins (below), or include individual files as needed -->\n",
    "    <script src=\"js/bootstrap.min.js\"></script>\n",
    "\n",
    "    <div class=\"container\">\n",
    "        <div class=\"page-header\">\n",
    "            <h3>About Me</h3>\n",
    "        </div>\n",
    "        <div class=\"page-info\">\n",
    "\n",
    "A web developer with experience in a variety of exciting projects, with the most up-to-date and relevant programming foundations available. My wide experience in\n",
    "a diversity of technologies guides me with the best way to get your business success.\n",
    "My interest in academic leads me to research in the field of NLP(Natural Language \n",
    "Processing). Other than the knowledge in CS/IT, I'm also a broad learner who loves \n",
    "to read each and every kind of books.\n",
    "        </div>\n",
    "    </div>\n",
    "</body>\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們可以透過正規表示法來移除HTML標籤："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About Me\n",
      "        \n",
      "        \n",
      "\n",
      "A web developer with experience in a variety of exciting projects, with the most up-to-date and relevant programming foundations available. My wide experience in\n",
      "a diversity of technologies guides me with the best way to get your business success.\n",
      "My interest in academic leads me to research in the field of NLP(Natural Language \n",
      "Processing). Other than the knowledge in CS/IT, I'm also a broad learner who loves \n",
      "to read each and every kind of books.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = re.sub(\"<[^>]+>\", \"\", text).strip()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們可以清楚地看到，在標題(About)和文字之間有一些跳行符號。在我們進行記號化(Tokenize)之前，讓我們先把這些跳行符號取代成空格吧。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['About Me\\n        \\n        ', \"A web developer with experience in a variety of exciting projects, with the most up-to-date and relevant programming foundations available. My wide experience in\\na diversity of technologies guides me with the best way to get your business success.\\nMy interest in academic leads me to research in the field of NLP(Natural Language \\nProcessing). Other than the knowledge in CS/IT, I'm also a broad learner who loves \\nto read each and every kind of books.\"]\n",
      "---------------------------\n",
      "A web developer with experience in a variety of exciting projects, with the most up-to-date and relevant programming foundations available. My wide experience in a diversity of technologies guides me with the best way to get your business success. My interest in academic leads me to research in the field of NLP(Natural Language  Processing). Other than the knowledge in CS/IT, I'm also a broad learner who loves  to read each and every kind of books.\n"
     ]
    }
   ],
   "source": [
    "text = text.split(\"\\n\\n\")\n",
    "print(text)\n",
    "print(\"---------------------------\")\n",
    "text = text[1].replace(\"\\n\", \" \")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接著，我們可以把文件分割成句子。雖然用過Python的朋友都知道可以單純的用.split()來處理現在這個例子，但我們還是試著用NLTK提供的句子分割器，為了因應未來可能要處理之更難的文字。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A web developer with experience in a variety of exciting projects, with the most up-to-date and relevant programming foundations available.', 'My wide experience in a diversity of technologies guides me with the best way to get your business success.', 'My interest in academic leads me to research in the field of NLP(Natural Language  Processing).', \"Other than the knowledge in CS/IT, I'm also a broad learner who loves  to read each and every kind of books.\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# 上面都是為了擷取文字\n",
    "\n",
    "# 2. 架構分割(Structure Segment) : 文章分割成句子\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "sent_segmenter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "sentences = sent_segmenter.tokenize(text)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了分割器，NLTK也能幫助字詞記號化。我們將範例文件中的第一個句子分別用python split和NLTK透過正規表示法寫出來的記號器做個比較吧！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'web', 'developer', 'with', 'experience', 'in', 'a', 'variety', 'of', 'exciting', 'projects', ',', 'with', 'the', 'most', 'up', '-', 'to', '-', 'date', 'and', 'relevant', 'programming', 'foundations', 'available', '.']\n",
      "------------\n",
      "['A', 'web', 'developer', 'with', 'experience', 'in', 'a', 'variety', 'of', 'exciting', 'projects,', 'with', 'the', 'most', 'up-to-date', 'and', 'relevant', 'programming', 'foundations', 'available.']\n"
     ]
    }
   ],
   "source": [
    "# 3. 記號化(Tokenize)            : 句子分割成一個一個的單字 list\n",
    "word_tokenizer = nltk.tokenize.regexp.WordPunctTokenizer()\n",
    "\n",
    "tokenized_sentence = word_tokenizer.tokenize(sentences[0])\n",
    "print(tokenized_sentence)\n",
    "# tokenized_sentence 等等會再丟入 lemmatization和stemming 處理\n",
    "print(\"------------\")\n",
    "# 比對與 tokenized_sentence 的差別\n",
    "print(sentences[0].split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 有跳錯 因此下載此套件\n",
    "import nltk\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK記號器能夠正確地將逗點和\"up-to-date\"這樣的自分割出來。當然，這樣的功能有時候是幫助我們的，在一些應用上這功能反而不是我們所希望發生的。\n",
    "\n",
    "接著，我們來測試Lemmatization。NLTK也有lemmatizer，在使用時上通常會需要先知道句子的詞性標注。在這個範例中，我們簡化這流程，先將輸入的文字用動詞來lemmatize，若發現文字沒有發生變化，我們再用名詞的lemmatizer來試試看。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'web', 'developer', 'with', 'experience', 'in', 'a', 'variety', 'of', 'excite', 'project', ',', 'with', 'the', 'most', 'up', '-', 'to', '-', 'date', 'and', 'relevant', 'program', 'foundation', 'available', '.']\n"
     ]
    }
   ],
   "source": [
    "# 4.單字正規化處理\n",
    "# 方法一 lemma會盡可能把恢復成字典上有的字\n",
    "# 這個 lib 還真的有在更新，我看跟之前輸出的切割方式不一樣\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word,'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word,'n')\n",
    "    return lemma\n",
    "\n",
    "print([lemmatize(token) for token in tokenized_sentence])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "現在我們也來試試看Stemming，我們使用NLTK內建的Porter Stemmer："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'web', 'develop', 'with', 'experi', 'in', 'a', 'varieti', 'of', 'excit', 'project', ',', 'with', 'the', 'most', 'up', '-', 'to', '-', 'date', 'and', 'relev', 'program', 'foundat', 'avail', '.']\n"
     ]
    }
   ],
   "source": [
    "# 方法二 stemming則會把文字的後置 (suffix)整個切掉而不在意切掉後的字是不是字典上有的字\n",
    "# developer -> develop 但我覺得 er 代表人 應該也是很重要的資訊\n",
    "# 好處是 如果要計算各種字出現的頻率 使用的空間比較少 且通常意思是相近的\n",
    "\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "print([stemmer.stem(token) for token in tokenized_sentence])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "大家可以觀察看看，在進行lemmatization和stemming之前的文字和之後的文字分別有哪些變化！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
