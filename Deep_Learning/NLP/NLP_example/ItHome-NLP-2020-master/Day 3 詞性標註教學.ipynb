{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 3: 詞性標註教學"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在NLTK上有許多已經手動進行詞性標注的文集了。今天的實作，我們會使用Penn Treebank文集(the Penn Treebank Corpus)以及Brown文集(the Brown Corpus)。其中Penn Treebank中搜集了許多華爾街日報的文章(Wall Street Journal)，而Brown中多數的文字和文學有關。在以下這格中我們下載Penn Treebank以及Brown這兩個文集，並且測試了這兩個文集中的第一個句子。\".tagged_sents()\"提取了詞性標註過的句子(sents = sentences)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n",
      "[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL'), ('said', 'VBD'), ('Friday', 'NR'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'IN'), (\"Atlanta's\", 'NP$'), ('recent', 'JJ'), ('primary', 'NN'), ('election', 'NN'), ('produced', 'VBD'), ('``', '``'), ('no', 'AT'), ('evidence', 'NN'), (\"''\", \"''\"), ('that', 'CS'), ('any', 'DTI'), ('irregularities', 'NNS'), ('took', 'VBD'), ('place', 'NN'), ('.', '.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to /Users/hyhu/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package brown to /Users/hyhu/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import treebank, brown\n",
    "nltk.download('treebank')\n",
    "nltk.download('brown')\n",
    "\n",
    "print(treebank.tagged_sents()[0])\n",
    "print(brown.tagged_sents()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在NLTK中，文字和標註的組合是以tuple的方式儲存的。然而在實作上，詞性標註常以\"word/tag\"的方式顯示，例如\"Pierre/NNP\", \"the/DT\"。NNP是指專有名詞、DT則為定冠詞，完整的標籤列表大家可以參考：https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html 。\n",
    "\n",
    "值得注意的是，這兩個文集並不是使用相同的標註方式。同樣是\"the\"，Brown把它標註成\"AT\"(Article，冠詞），而在Penn Treebank中則被標註為\"DT(Determiner，定冠詞）。好消息是，在NLTK中我們也可以把他們都轉換成Universal標註方式。https://universaldependencies.org/u/pos/ 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Pierre', 'NOUN'), ('Vinken', 'NOUN'), (',', '.'), ('61', 'NUM'), ('years', 'NOUN'), ('old', 'ADJ'), (',', '.'), ('will', 'VERB'), ('join', 'VERB'), ('the', 'DET'), ('board', 'NOUN'), ('as', 'ADP'), ('a', 'DET'), ('nonexecutive', 'ADJ'), ('director', 'NOUN'), ('Nov.', 'NOUN'), ('29', 'NUM'), ('.', '.')]\n",
      "[('The', 'DET'), ('Fulton', 'NOUN'), ('County', 'NOUN'), ('Grand', 'ADJ'), ('Jury', 'NOUN'), ('said', 'VERB'), ('Friday', 'NOUN'), ('an', 'DET'), ('investigation', 'NOUN'), ('of', 'ADP'), (\"Atlanta's\", 'NOUN'), ('recent', 'ADJ'), ('primary', 'NOUN'), ('election', 'NOUN'), ('produced', 'VERB'), ('``', '.'), ('no', 'DET'), ('evidence', 'NOUN'), (\"''\", '.'), ('that', 'ADP'), ('any', 'DET'), ('irregularities', 'NOUN'), ('took', 'VERB'), ('place', 'NOUN'), ('.', '.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /Users/hyhu/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('universal_tagset')\n",
    "print(treebank.tagged_sents(tagset=\"universal\")[0])\n",
    "print(brown.tagged_sents(tagset=\"universal\")[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "知道了詞性標註的基本規則之後，我們開始開發一個Unigram Tagger吧！首先，我們需要先記錄每一個字形是出現各種詞性的頻率。我們可以將它存在python資料結構中，dictionary中的dictionary（事實上這不是最有效率的存法 :D）。雖然前面也介紹Brown Corpus，但從這邊開始我們專注於使用Penn Treebank的標籤規則。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "POS_dict = defaultdict(dict)\n",
    "for word_pos_pair in treebank.tagged_words():\n",
    "    word = word_pos_pair[0].lower() # 正規化成小寫\n",
    "    POS = word_pos_pair[1]\n",
    "    POS_dict[word][POS] = POS_dict[word].get(POS,0) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "取一些字來看看他們怎麼表現多個詞性標註，以及每個詞性在文集中的分布狀況："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "{'NN': 8, 'VB': 2}\n",
      "forecast\n",
      "{'NN': 4, 'VBP': 1, 'VBD': 1, 'VBN': 1}\n",
      "recorded\n",
      "{'VBN': 1, 'VBD': 1}\n",
      "4\n",
      "{'CD': 15, 'LS': 1}\n",
      "keep\n",
      "{'VB': 15, 'VBP': 1}\n",
      "pace\n",
      "{'NN': 6, 'NNP': 1}\n",
      "rival\n",
      "{'JJ': 1, 'NN': 4}\n",
      "announced\n",
      "{'VBD': 14, 'VBN': 3}\n",
      "advertising\n",
      "{'NN': 10, 'VBG': 3}\n",
      "plan\n",
      "{'NN': 45, 'NNP': 2, 'VB': 1, 'VBP': 2}\n",
      "ad\n",
      "{'NN': 28, 'NNP': 1}\n",
      "post\n",
      "{'NNP': 1, 'VB': 2, 'NN': 4}\n",
      "second\n",
      "{'JJ': 16, 'NNP': 2}\n",
      "offered\n",
      "{'VBN': 11, 'VBD': 16}\n",
      "plans\n",
      "{'NNS': 17, 'VBZ': 18, 'VBP': 1}\n",
      "give\n",
      "{'VBP': 6, 'VB': 15}\n",
      "spending\n",
      "{'NN': 25, 'VBG': 2}\n",
      "become\n",
      "{'VBN': 7, 'VB': 15, 'VBP': 1}\n",
      "news\n",
      "{'NN': 24, 'NNP': 6}\n",
      "world\n",
      "{'NNP': 13, 'NN': 24}\n",
      "5\n",
      "{'CD': 18, 'LS': 1}\n",
      "cost\n",
      "{'VB': 4, 'NN': 12}\n",
      "lowered\n",
      "{'VBD': 2, 'VBN': 2}\n",
      "circulation\n",
      "{'NN': 10, 'NNP': 1}\n",
      "lower\n",
      "{'JJR': 30, 'RBR': 1}\n",
      "costs\n",
      "{'VBZ': 4, 'NNS': 22}\n",
      "yet\n",
      "{'RB': 17, 'CC': 6}\n",
      "credit\n",
      "{'NNP': 2, 'NN': 22}\n",
      "meet\n",
      "{'VBP': 3, 'VB': 9}\n",
      "exceed\n",
      "{'VBP': 1, 'VB': 3}\n",
      "long\n",
      "{'RB': 11, 'JJ': 16, 'NNP': 1}\n",
      "spent\n",
      "{'VBD': 4, 'VBN': 4}\n",
      "attempt\n",
      "{'NN': 8, 'VB': 2}\n",
      "decline\n",
      "{'NN': 14, 'VBP': 3, 'VB': 4}\n",
      "drop\n",
      "{'NN': 15, 'VB': 4}\n",
      "what\n",
      "{'WP': 68, 'WDT': 3}\n"
     ]
    }
   ],
   "source": [
    "for word in list(POS_dict.keys())[900:1000]:\n",
    "    if len(POS_dict[word]) > 1:\n",
    "        print(word)\n",
    "        print(POS_dict[word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在這裡我們能觀察到一些常見的歧義會發生在名詞和動詞之間(<i>plans</i>, <i>decline</i>, <i>cost</i>)；在動詞之間，過去式和過去分詞也會發生同樣的問題(<i>announced</i>, <i>offered</i>, <i>spent</i>).\n",
    "\n",
    "為了開發出我們的第一個標註器(Unigram Tagger)，我們只需要為每個詞選出最常見的詞性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('You', 'NN'), ('better', 'JJR'), ('start', 'VB'), ('swimming', 'NN'), ('or', 'CC'), ('sink', 'VB'), ('like', 'IN'), ('a', 'DT'), ('stone', 'NN'), (',', ','), ('cause', 'NN'), ('the', 'DT'), ('times', 'NNS'), ('they', 'PRP'), ('are', 'VBP'), ('a', 'DT'), ('-', ':'), ('changing', 'VBG'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "tagger_dict = {}\n",
    "for word in POS_dict:\n",
    "    tagger_dict[word] = max(POS_dict[word],key=lambda x: POS_dict[word][x])\n",
    "\n",
    "def tag(sentence):\n",
    "    return [(word,tagger_dict.get(word,\"NN\")) for word in sentence]\n",
    "\n",
    "example_sentence = \"\"\"You better start swimming or sink like a stone , cause the times they are a - changing .\"\"\".split() \n",
    "print(tag(example_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因為不是每一個字都是我們在training set中看過的字，所以遇到沒有看過的字，我們會自動標註成名詞\"NN\"。我們可以觀察到這樣的方法雖然會有一些問題，例如\"swimming\"在該是動詞，卻因此被標註成了名詞。然而總體而言，這樣標註的成效還挺不錯的。 \n",
    "\n",
    "NLTK也有內建的N-gram tagger，我們可以使用內建的Unigram(1-gram)和Bigram(2-gram) Tagger。首先，需要將文集切割成訓練集和測試集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練集:測試集 = 9:1\n",
    "size = int(len(treebank.tagged_sents()) * 0.9)\n",
    "train_sents = treebank.tagged_sents()[:size] \n",
    "test_sents = treebank.tagged_sents()[size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們先來比對預設的Unigram和Bigram Tagger。NLTK裡面所有的標註器都有評價功能，藉此回傳測試集運行在這個訓練模型的準確率(accuracy)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8627989821882952\n",
      "[('You', 'PRP'), ('better', 'JJR'), ('start', 'VB'), ('swimming', None), ('or', 'CC'), ('sink', 'VB'), ('like', 'IN'), ('a', 'DT'), ('stone', 'NN'), (',', ','), ('cause', 'NN'), ('the', 'DT'), ('times', 'NNS'), ('they', 'PRP'), ('are', 'VBP'), ('a', 'DT'), ('-', ':'), ('changing', 'VBG'), ('.', '.')]\n",
      "0.13455470737913486\n",
      "[('You', 'PRP'), ('better', None), ('start', None), ('swimming', None), ('or', None), ('sink', None), ('like', None), ('a', None), ('stone', None), (',', None), ('cause', None), ('the', None), ('times', None), ('they', None), ('are', None), ('a', None), ('-', None), ('changing', None), ('.', None)]\n"
     ]
    }
   ],
   "source": [
    "from nltk import UnigramTagger, BigramTagger\n",
    "\n",
    "unigram_tagger = UnigramTagger(train_sents)\n",
    "bigram_tagger = BigramTagger(train_sents)\n",
    "print(unigram_tagger.evaluate(test_sents))\n",
    "print(unigram_tagger.tag(example_sentence))\n",
    "print(bigram_tagger.evaluate(test_sents))\n",
    "print(bigram_tagger.tag(example_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在這裡Unigram Tagger的效果好太多了。原因很明顯，因為Bigram Tagger並沒有足夠的資料來觀察前後文的關係，更糟的是，一旦一個詞的詞性判斷被判定成\"None\"，後面整句話也都會失敗。為了解決問題，我們需要為Bigram Tagger加上退避(backoffs)。關於退避，我們在未來講到N-gram語言模型和Smoothing時會詳細討論，現在我們就先預設那些\"None\"的字為\"NN\"吧！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8905852417302799\n",
      "[('You', 'PRP'), ('better', 'JJR'), ('start', 'VB'), ('swimming', 'NN'), ('or', 'CC'), ('sink', 'VB'), ('like', 'IN'), ('a', 'DT'), ('stone', 'NN'), (',', ','), ('cause', 'VB'), ('the', 'DT'), ('times', 'NNS'), ('they', 'PRP'), ('are', 'VBP'), ('a', 'DT'), ('-', ':'), ('changing', 'VBG'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import DefaultTagger\n",
    "\n",
    "default_tagger = DefaultTagger(\"NN\")\n",
    "unigram_tagger = UnigramTagger(train_sents,backoff=default_tagger)\n",
    "bigram_tagger = BigramTagger(train_sents,backoff=unigram_tagger)\n",
    "\n",
    "print(bigram_tagger.evaluate(test_sents))\n",
    "print(bigram_tagger.tag(example_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "藉由退避方法，我們將Bigram的資訊加到Unigram之上，準確率也有了3%的提升。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
