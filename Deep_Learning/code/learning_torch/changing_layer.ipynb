{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee5ffbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.onnx \n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b507abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.densenet201(num_classes=1000, pretrained=True) # 0.15 pretrained 參數會移除\n",
    "# model = models.densenet201(num_classes=1000, weights=DenseNet201_Weights.DEFAULT) # 未來 # 雖然我還不知道 DenseNet201_Weights 變數怎麼 import\n",
    "\n",
    "# 首先印出架構找出要替換的 Layer\n",
    "# (本次的目的是要替換 FCN 成 segmentation layer)\n",
    "# 因為 model 架構太大所以一定要用 ipynb 顯示\n",
    "# print(\"net construct :\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801c6bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定\n",
    "input_size = (1, 480, 360)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c699b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我先直接 segmentation 看能不能輸出\n",
    "# model.classifier = nn.ConvTranspose2d(1920, 1, 4, stride=34, padding=0)\n",
    "# model.classifier = nn.ConvTranspose2d(1920, 1, 4, stride=2, padding=1)\n",
    "# model.classifier = nn.Sequential(\n",
    "#     nn.ConvTranspose2d(1920, 1, 4, stride=34, padding=0), # [1, 1, 480, 344]\n",
    "# )\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.ConvTranspose2d(1920, 1, kernel_size=2, stride=2, padding=2),\n",
    "    nn.ConvTranspose2d(1, 1, kernel_size=2, stride=2, padding=0),\n",
    "    # nn.ConvTranspose2d(1, 1, kernel_size=3, stride=2, padding=1, output_padding=2),\n",
    "    # nn.ConvTranspose2d(1, 1, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "    # nn.ConvTranspose2d(1, 1, kernel_size=2, stride=2, padding=2),\n",
    ")\n",
    "dummy_input = torch.randn(1, 3, 480, 360, requires_grad=True)\n",
    "\n",
    "# from torch import Tensor\n",
    "# def forward(self, x: Tensor) -> Tensor:\n",
    "#     features = self.features(x)\n",
    "#     out = F.relu(features, inplace=True)\n",
    "#     out = F.adaptive_avg_pool2d(out, (1, 1))\n",
    "#     out = torch.flatten(out, 1)\n",
    "#     out = self.classifier(out)\n",
    "#     return out\n",
    "\n",
    "# model.forward = \n",
    "\n",
    "y = model(Variable(dummy_input))\n",
    "print(y.shape)\n",
    "# print(\"net construct :\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e40b7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測試是否能正常輸出\n",
    "\n",
    "# set the model to inference mode \n",
    "# 不启用 Batch Normalization 和 Dropout (所以在做辨識之前一定要先呼叫這個)\n",
    "# https://cloud.tencent.com/developer/article/1819853\n",
    "model.eval() \n",
    "\n",
    "# Let's create a dummy input tensor  \n",
    "# 這個是為了要讓 model 知道 input 大小是多少\n",
    "dummy_input = torch.randn(1, 3, 480, 360, requires_grad=True)\n",
    "\n",
    "# Export the model   \n",
    "torch.onnx.export(model,         # model being run \n",
    "        dummy_input,       # model input (or a tuple for multiple inputs) \n",
    "        \"densenet201.onnx\",       # where to save the model  \n",
    "        export_params=True,  # store the trained parameter weights inside the model file \n",
    "        opset_version=10,    # the ONNX version to export the model to \n",
    "        do_constant_folding=True,  # whether to execute constant folding for optimization \n",
    "        input_names = ['modelInput'],   # the model's input names \n",
    "        output_names = ['modelOutput'], # the model's output names \n",
    "        dynamic_axes={'modelInput' : {0 : 'batch_size'},    # variable length axes \n",
    "                            'modelOutput' : {0 : 'batch_size'}}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d1a863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測試 predict 輸出大小\n",
    "# res = model.forward(dummy_input)\n",
    "res = model(dummy_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba32eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(res))\n",
    "print(len(res))\n",
    "# print(res.mH)\n",
    "# print(res.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取结果\n",
    "pred = np.array(res.data.cpu()[0])[0]\n",
    "# 处理结果\n",
    "pred[pred >= 0.5] = 255\n",
    "pred[pred < 0.5] = 0\n",
    "# 保存图片\n",
    "# cv2.imwrite(save_res_path, pred)\n",
    "plt.imshow(pred)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('pytorch_cpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "8f8969ddbe274ce60ce702c2d151a1feb5bfbd8068634f4d50f57ebbb3fdb0e2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
