imporet sklearn # Scikit-learn

Confusion matrix (混淆矩陣) 
    from sklearn.metrics import confusion_matrix
        可以直接印出 Confusion matrix
    用來看準確率的
        看主要錯誤原因是誤判(不應該判但判到)還是漏判(應該要判沒判到)
    分類 : 
        True Positive TP (真陽性) 實際為真，模型也預測為真 (預測正確)。
        True Negative TN (真陰性) 實際為假，模型也預測為假 (預測正確)。
        False Positive FP (偽陽性) 實際為假，模型卻預測為真 (型一錯誤 Type I Error)。
        False Negative FN (偽陰性) 實際為真，模型卻預測為負 (型二錯誤 Type II Error)。
    計算出來的準確率 :
        Accuracy:  (TP+TN) / all 
            The number of correct predictions (true positives + true negatives) divided by the total number of predictions.
        Precision: TP/辨識結果為P(TP+FP)  (不想要錯的辨識成正確的) 避免沒病驗出有病
            The number of the cases classified as positive that are actually positive: the number of true positives divided by (the number of true positives plus false positives).
        Recall: TP/實際結果為P(TP+FN)     (不想要正確的辨識成錯的) 避免有病卻沒驗出病
            The fraction of positive cases correctly identified: the number of true positives divided by (the number of true positives plus false negatives).
        Fβ Score 
            (1 + β^2) * (Precision*Recall) / (β^2*Precision + Recall)
            F1 Score: 2*(Precision*Recall) / (Precision + Recall)  (balance precision and recall)
                An overall metric that essentially combines precision and recall.
            F0.5 Score: (precision is more important than recall)
                (1 + 0.5^2) * (Precision*Recall) / (0.5^2*Precision + Recall)
            F2 Score:   (recall is more important than precision)
                (1 + 2^2) * (Precision*Recall) / (2^2*Precision + Recall)

ROC curve and AUC scores
    https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc
    ROC(receiver operating characteristic) Curve
        跟據 threshold 不同畫出來的 TP與FP 比例
        from sklearn.metrics import roc_curve
    AUC(Area Under the Curve) scores
        計算 ROC Curve 底下的面積
            面積愈大愈好
        from sklearn.metrics import roc_auc_score

PR curve (Precision-Recall Curve)
    通常用在 data 類別數量不平衡 的時候
    找到 Precision 跟 Recall 之間的平衡

都是準確率的計算方法 反正愈大愈好
    AP (Average Precision)
    AR (Average Recall)
        IOU 的準確率
    mAP (mean Average Precision)

-- <各種計算 Loss 的方法> ----------------------------------------------------------------------
cost = loss == residuals(殘差) == errors
    residuals = (ŷ - y)
表示方法 :
    y - ŷ = diff or loss (真正的結果 - model 計算結果)

cost funtion == loss function 
    L1 (Manhattan Norm) (不會開根號) 
        Mean absolute error, MAE(平均絕對值誤差) (L1 loss)
            sum(abs(預測答案 - 正確答案)) / 個數
            nn.MSELoss()
    L2 系列(其實都通用 也沒那麼嚴謹)
    表示方法 : arg min B (|y - y'|) L2 norm
        SSE (Sum of Squared Errors) 
            sum((預測答案 - 正確答案)^2) -> (平方 相加 沒有平均)
        L2 norm (Euclidean Norm)
            L2 norm 是 SSE 開根號之後的結果 -> (平方 相加 開根號)
            因為有開根號 所以幾何意義上是 距離
        Mean square error, MSE (均方差) (L2 loss)
            SSE / 個數 -> (平方 相加 "平均")
            EX:
                如果是 linear regression
                    可以用 Ordinary Least Squares (OLS) 計算 MSE 的最佳擬合線的方法
                    (X^T X)^(-1) X^T y
        root mean squared error, RMSE
            RMSE 是 MSE 開根號之後的結果
    mean absolute percentage error (MAPE)
        sum_i( |y - y'|/y ) / n
    correlation coefficient (R2)
        1 - [ sum_i((y - y')^2) / sum_i((y - y'_mean)^2) ]
        how closely the predicted values match the actual values


loss 的 類別
    aleatoric uncertainty 偶然不確定性 == 資料不確定性 (Data Uncertainty)
        不能被縮小 是隨機發生的誤差
        只有 RL 可以控制
            處理 Exploitation (利用) 與 Safety (安全) 之間的平衡
    epistemic uncertainty 認知不確定性 == 模型不確定性 (Model Uncertainty)
        只能被縮到 aleatoric uncertainty 這麼大

    residuals errors = aleatoric uncertainty + epistemic uncertainty
                     = ( Y - fB*(x) )        + ( fB*(x) - fB̂*(x) )
        fB*(x) is the best solution

    error = Bias^2 + Variance + AU
        but usually Bias and variance are inversely related, forming a trade-off
        Bias    ：模型預測 與 真實值之間的"平均值"的差距
            high bias model : bad on validation and training data (underfitting)
                generalizing : should increase the model complexity or adding more features
        Variance：模型預測隨"不同訓練資料"的差距
            high variance model : bad on validation but good on training data (overfitting)
                generalizing : should increase the data complexity
    overfitting : 模型複雜度過高 + 資料不足 <-> underfitting
        主要是model造成的，training過度只是加速或加劇 overfitting


-- < 驗證方法 > ----------------------------------------------------------------------
    x-fold cross-validation 把data拆成x組 每次會把其中一組當作 test data 進行訓練

-- < code > ----------------------------------------------------------------------
keras 有的 loss 函數
    https://keras.io/zh/losses/

使用範例
from keras import losses
model.compile(loss=losses.mean_squared_error, optimizer='sgd')
或直接用名稱
model.compile(loss=loss = "categorical_crossentropy", optimizer='sgd')

loss=losses.mean_squared_error
==============================================================

Dice Loss
    醫學圖像常用
        https://blog.csdn.net/JMU_Ma/article/details/97533768 (未看)